## Copyright Broadcom, Inc. All Rights Reserved.
## SPDX-License-Identifier: APACHE-2.0

## @section Global parameters
## Global Docker image parameters
## Please, note that this will override the image parameters, including dependencies, configured to use the global value
## Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass
##

## @param global.imageRegistry Global Docker image registry
## @param global.imagePullSecrets Global Docker registry secret names as an array
## @param global.defaultStorageClass Global default StorageClass for Persistent Volume(s)
##
global:
  imageRegistry: ""
  ## E.g.
  ## imagePullSecrets:
  ##   - myRegistryKeySecretName
  ##
  imagePullSecrets: []
  defaultStorageClass: ""
  ## Security parameters
  ##
  security:
    ## @param global.security.allowInsecureImages Allows skipping image verification
    allowInsecureImages: false
  ## Compatibility adaptations for Kubernetes platforms
  ##
  compatibility:
    ## Compatibility adaptations for Openshift
    ##
    openshift:
      ## @param global.compatibility.openshift.adaptSecurityContext Adapt the securityContext sections of the deployment to make them compatible with Openshift restricted-v2 SCC: remove runAsUser, runAsGroup and fsGroup and let the platform use their allowed default IDs. Possible values: auto (apply if the detected running cluster is Openshift), force (perform the adaptation always), disabled (do not perform adaptation)
      ##
      adaptSecurityContext: auto
## @section Common parameters
##

## @param kubeVersion Override Kubernetes version
##
kubeVersion: ""
## @param nameOverride String to partially override common.names.fullname
##
nameOverride: ""
## @param fullnameOverride String to fully override common.names.fullname
##
fullnameOverride: ""
## @param commonLabels Labels to add to all deployed objects
##
commonLabels: {}
## @param commonAnnotations Annotations to add to all deployed objects
##
commonAnnotations: {}
## @param clusterDomain Kubernetes cluster domain name
##
clusterDomain: cluster.local
## @param extraDeploy Array of extra objects to deploy with the release
##
extraDeploy: []
## @param enableServiceLinks Whether information about services should be injected into all pods' environment variable
## The environment variables injected by service links are not used, but can lead to slow boot times or slow running of the scripts when there are many services in the current namespace.
## If you experience slow pod startups or slow running of the scripts you probably want to set this to `false`.
##
enableServiceLinks: false
## Enable diagnostic mode in the deployments/statefulsets
##
diagnosticMode:
  ## @param diagnosticMode.enabled Enable diagnostic mode (all probes will be disabled and the command will be overridden)
  ##
  enabled: false
  ## @param diagnosticMode.command Command to override all containers in the deployments/statefulsets
  ##
  command:
    - sleep
  ## @param diagnosticMode.args Args to override all containers in the deployments/statefulsets
  ##
  args:
    - infinity
## @section Common Milvus Parameters
##
milvus:
  ## Bitnami Milvus image
  ## ref: https://hub.docker.com/r/bitnami/milvus/tags/
  ## @param milvus.image.registry [default: REGISTRY_NAME] Milvus image registry
  ## @param milvus.image.repository [default: REPOSITORY_NAME/milvus] Milvus image repository
  ## @skip milvus.image.tag Milvus image tag (immutable tags are recommended)
  ## @param milvus.image.digest Milvus image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
  ## @param milvus.image.pullPolicy Milvus image pull policy
  ## @param milvus.image.pullSecrets Milvus image pull secrets
  ## @param milvus.image.debug Enable debug mode
  ##
  image:
    registry: docker.io
    repository: bitnami/milvus
    tag: 2.5.2-debian-12-r0
    digest: ""
    ## Specify a imagePullPolicy
    ## ref: https://kubernetes.io/docs/concepts/containers/images/#pre-pulled-images
    ##
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## e.g:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []
    ## Enable debug mode
    ##
    debug: false
  auth:
    ## @param milvus.auth.enabled enable Milvus authentication
    ##
    enabled: false
    ## @param milvus.auth.username Milvus username
    ##
    username: user
    ## @param milvus.auth.password Milvus username password
    ##
    password: ""
    ## @param milvus.auth.rootPassword Milvus root password
    ##
    rootPassword: ""
    ## @param milvus.auth.existingSecret Name of a secret containing the Milvus password
    ##
    existingSecret: ""
    ## @param milvus.auth.existingSecretPasswordKey Name of the secret key containing the Milvus password
    ##
    existingSecretPasswordKey: ""
  ## @param milvus.defaultConfig [string] Milvus components default configuration
  ##
  defaultConfig: |
    # etcd configuration
    etcd:
      endpoints:
        {{- if .Values.etcd.enabled  }}
          {{- $replicas := $.Values.etcd.replicaCount | int }}
          {{- range $i, $_e := until $replicas }}
        - {{ printf "%s://%s-%d.%s:%v" (ternary "https" "http" $.Values.etcd.auth.client.secureTransport) (include "milvus.etcd.fullname" $ ) $i (include "milvus.etcd.headlessServiceName" $) ( include "milvus.etcd.port" $ ) }}          {{- end }}
        {{- else }}
        {{- range $node := .Values.externalEtcd.servers }}
        - {{ ternary "https" "http" $.Values.externalEtcd.tls.enabled }}://{{ printf "%s:%v" $node (include "milvus.etcd.port" $) }}
        {{- end }}
      ssl:
        enabled: {{ .Values.externalEtcd.tls.enabled }}
        tlsCert: /opt/bitnami/milvus/configs/cert/etcd/client/{{ .Values.externalEtcd.tls.cert }}
        tlsKey: /opt/bitnami/milvus/configs/cert/etcd/client/{{ .Values.externalEtcd.tls.key }}
        tlsCACert: /opt/bitnami/milvus/configs/cert/etcd/client/{{ .Values.externalEtcd.tls.caCert }}
        {{- end }}
    metastore:
      type: etcd

    # S3 configuration
    minio:
      address: {{ include "milvus.s3.host" . }}
      port: {{ include "milvus.s3.port" . }}
      accessKeyID: {{ print "{{ MILVUS_S3_ACCESS_ID }}" | quote }}
      secretAccessKey: {{ print "{{ MILVUS_S3_SECRET_ACCESS_KEY }}" | quote }}
      useSSL: {{ include "milvus.s3.useSSL" . }}
      bucketName: {{ include "milvus.s3.bucket" . }}
      rootPath: {{ include "milvus.s3.rootPath" . }}
      useIAM: {{ include "milvus.s3.useIAM" . }}
      {{- if not .Values.minio.enabled }}
      cloudProvider: {{ .Values.externalS3.cloudProvider }}
      iamEndpoint: {{ .Values.externalS3.iamEndpoint }}
      {{- end }}

    # Kafka configuration
    kafka:
      brokerList:
        {{- if .Values.kafka.enabled  }}
          {{- $brokerReplicas := $.Values.kafka.broker.replicaCount | int }}
          {{- $controllerReplicas := 0 }}
          {{- if or (not .Values.kafka.kraft.enabled) (not .Values.kafka.controller.controllerOnly)}}
          {{- $controllerReplicas = $.Values.kafka.controller.replicaCount | int }}
          {{- end }}
          {{- range $i, $_e := until $brokerReplicas }}
        - {{ printf "%s-broker-%d.%s:%v" (include "milvus.kafka.fullname" $ ) $i (include "milvus.kafka.broker.headlessServiceName" $) ( include "milvus.kafka.port" $ ) }}
          {{- end }}
          {{- range $i, $_e := until $controllerReplicas }}
        - {{ printf "%s-controller-%d.%s:%v" (include "milvus.kafka.fullname" $ ) $i (include "milvus.kafka.controller.headlessServiceName" $) ( include "milvus.kafka.port" $ ) }}
          {{- end }}
        {{- else }}
        {{- range $node := .Values.externalKafka.servers }}
        - {{ printf "%s:%v" $node (include "milvus.kafka.port" $) }}
        {{- end }}
        {{- end }}
      securityProtocol: {{ include "milvus.kafka.securityProtocol" . }}
      {{- if include "milvus.kafka.authEnabled" . }}
      saslMechanisms: {{ include "milvus.kafka.saslMechanisms" . }}
      saslUsername: {{ include "milvus.kafka.user" . }}
      saslPassword: {{ print "{{ MILVUS_KAFKA_PASSWORD }}" | quote }}
      {{- end }}

    # Data coordinator
    dataCoord:
      address: {{ include "milvus.data-coordinator.fullname" . }}
      port: {{ .Values.dataCoord.service.ports.grpc }}

    # Root coordinator
    rootCoord:
      address: {{ include "milvus.root-coordinator.fullname" . }}
      port: {{ .Values.rootCoord.service.ports.grpc }}

    # Index coordinator
    indexCoord:
      address: {{ include "milvus.index-coordinator.fullname" . }}
      port: {{ .Values.indexCoord.service.ports.grpc }}

    # Query coordinator
    queryCoord:
      address: {{ include "milvus.query-coordinator.fullname" . }}
      port: {{ .Values.queryCoord.service.ports.grpc }}

    # Data node
    dataNode:
      port: {{ .Values.dataNode.service.ports.grpc }}

    # Index node
    indexNode:
      port: {{ .Values.indexNode.service.ports.grpc }}

    # Query node
    queryNode:
      port: {{ .Values.queryNode.service.ports.grpc }}

    proxy:
      port: {{ .Values.proxy.service.ports.grpc }}
      accessLog:
        localPath: /dev
        filename: stdout
      http:
        enabled: true

    # Log configuration
    log:
      level: {{ ternary "debug" "info" .Values.milvus.image.debug }}
      stdout: true

    # Common configuration
    common:
      storageType: minio
      security:
        authorizationEnabled: {{ .Values.milvus.auth.enabled }}
        {{- if .Values.milvus.auth.enabled }}
        superUsers:
          - {{ .Values.milvus.auth.username }}
        {{- end }}
  ## @param milvus.extraConfig Extra configuration parameters
  ##
  extraConfig: {}
  ## @param milvus.existingConfigMap name of a ConfigMap with existing configuration for the default configuration
  ##
  existingConfigMap: ""
  ## @param milvus.extraConfigExistingConfigMap name of a ConfigMap with existing configuration for the Dashboard
  ##
  extraConfigExistingConfigMap: ""
initJob:
  ## @param initJob.forceRun Force the run of the credential job
  ##
  forceRun: false
  ## Bitnami PyMilvus image
  ## ref: https://hub.docker.com/r/bitnami/jwt-cli/tags/
  ## @param initJob.image.registry [default: REGISTRY_NAME] PyMilvus image registry
  ## @param initJob.image.repository [default: REPOSITORY_NAME/pymilvus] PyMilvus image repository
  ## @skip initJob.image.tag PyMilvus image tag (immutable tags are recommended)
  ## @param initJob.image.digest PyMilvus image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag image tag (immutable tags are recommended)
  ## @param initJob.image.pullPolicy PyMilvus image pull policy
  ## @param initJob.image.pullSecrets PyMilvus image pull secrets
  ##
  image:
    registry: docker.io
    repository: bitnami/pymilvus
    tag: 2.5.2-debian-12-r0
    digest: ""
    ## Specify a imagePullPolicy
    ## ref: http://kubernetes.io/docs/concepts/containers/images/#pre-pulled-images
    ##
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## e.g:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []
  ## @param initJob.enableDefaultInitContainers Deploy default init containers
  ##
  enableDefaultInitContainers: true
  ## @section TLS Client Configuration Parameters Connecting to Proxy
  ##
  tls:
    ## @param initJob.tls.existingSecret Name of the existing secret containing the TLS certificates for initJob.
    ## if proxy.tls.mode equals to 1, existingSecret should contain a server cert
    ## if proxy.tls.mode equals to 2, existingSecret should contain a client cert, a client key and a CA cert
    ##
    existingSecret: ""
    ## @param initJob.tls.cert The secret key from the existingSecret if 'cert' key different from the default (client.pem)
    ##
    cert: client.pem
    ## @param initJob.tls.key The secret key from the existingSecret if 'key' key different from the default (client.key)
    ## Milvus cannot decrypt key file with password, so now the key must not be encrypted
    ##
    key: client.key
    ## @param initJob.tls.caCert The secret key from the existingSecret if 'caCert' key different from the default (ca.pem)
    ##
    caCert: ca.pem
    ## @param initJob.tls.keyPassword Password to access the password-protected PEM key if necessary.
    ## Milvus cannot decrypt key file with password, so now the key must not be encrypted, value here must be empty
    ##
    keyPassword: ""
  ## @param initJob.backoffLimit set backoff limit of the job
  ##
  backoffLimit: 10
  ## @param initJob.extraVolumes Optionally specify extra list of additional volumes for the credential init job
  ##
  extraVolumes: []
  ## @param initJob.extraCommands Extra commands to pass to the generation job
  ##
  extraCommands: ""
  ## Configure Container Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
  ## @param initJob.containerSecurityContext.enabled Enabled containers' Security Context
  ## @param initJob.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
  ## @param initJob.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
  ## @param initJob.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
  ## @param initJob.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
  ## @param initJob.containerSecurityContext.privileged Set container's Security Context privileged
  ## @param initJob.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
  ## @param initJob.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
  ## @param initJob.containerSecurityContext.capabilities.drop List of capabilities to be dropped
  ## @param initJob.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
  ##
  containerSecurityContext:
    enabled: true
    seLinuxOptions: {}
    runAsUser: 1001
    runAsGroup: 1001
    runAsNonRoot: true
    privileged: false
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    capabilities:
      drop: ["ALL"]
    seccompProfile:
      type: "RuntimeDefault"
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param initJob.podSecurityContext.enabled Enabled credential init job pods' Security Context
  ## @param initJob.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
  ## @param initJob.podSecurityContext.sysctls Set kernel settings using the sysctl interface
  ## @param initJob.podSecurityContext.supplementalGroups Set filesystem extra groups
  ## @param initJob.podSecurityContext.fsGroup Set credential init job pod's Security Context fsGroup
  ##
  podSecurityContext:
    enabled: true
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## @param initJob.extraEnvVars Array containing extra env vars to configure the credential init job
  ## For example:
  ## extraEnvVars:
  ##  - name: GF_DEFAULT_INSTANCE_NAME
  ##    value: my-instance
  ##
  extraEnvVars: []
  ## @param initJob.extraEnvVarsCM ConfigMap containing extra env vars to configure the credential init job
  ##
  extraEnvVarsCM: ""
  ## @param initJob.extraEnvVarsSecret Secret containing extra env vars to configure the credential init job (in case of sensitive data)
  ##
  extraEnvVarsSecret: ""
  ## @param initJob.extraVolumeMounts Array of extra volume mounts to be added to the jwt Container (evaluated as template). Normally used with `extraVolumes`.
  ##
  extraVolumeMounts: []
  ## Container resource requests and limits
  ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param initJob.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if initJob.resources is set (initJob.resources is recommended for production).
  ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
  ##
  resourcesPreset: "micro"
  ## @param initJob.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## Configure extra options for Data Coordinator containers' liveness, readiness and startup probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
  ## @param initJob.livenessProbe.enabled Enable livenessProbe on init job
  ## @param initJob.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param initJob.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param initJob.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param initJob.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param initJob.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param initJob.readinessProbe.enabled Enable readinessProbe on init job
  ## @param initJob.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param initJob.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param initJob.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param initJob.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param initJob.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param initJob.startupProbe.enabled Enable startupProbe on Data Coordinator containers
  ## @param initJob.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param initJob.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param initJob.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param initJob.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param initJob.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param initJob.customLivenessProbe Custom livenessProbe that overrides the default one
  ##
  customLivenessProbe: {}
  ## @param initJob.customReadinessProbe Custom readinessProbe that overrides the default one
  ##
  customReadinessProbe: {}
  ## @param initJob.customStartupProbe Custom startupProbe that overrides the default one
  ##
  customStartupProbe: {}
  ## @param initJob.automountServiceAccountToken Mount Service Account token in pod
  ##
  automountServiceAccountToken: false
  ## @param initJob.hostAliases Add deployment host aliases
  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
  ##
  hostAliases: []
  ## @param initJob.annotations [object] Add annotations to the job
  ##
  annotations:
    helm.sh/hook: post-install
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    # This should be executed after the minio provisioning job
    helm.sh/hook-weight: "10"
  ## @param initJob.podLabels Additional pod labels
  ## Ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  ##
  podLabels: {}
  ## @param initJob.podAnnotations Additional pod annotations
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  podAnnotations: {}
  ## Network Policy configuration
  ## ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param initJob.networkPolicy.enabled Enable creation of NetworkPolicy resources
    ##
    enabled: true
    ## @param initJob.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param initJob.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraIngress: []
    ## @param initJob.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param initJob.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces
    ## @param initJob.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces
    ##
    ingressNSMatchLabels: {}
    ingressNSPodMatchLabels: {}
## @section Data Coordinator Deployment Parameters
##
dataCoord:
  ## @param dataCoord.enabled Enable Data Coordinator deployment
  ##
  enabled: true
  ## @param dataCoord.extraEnvVars Array with extra environment variables to add to data coordinator nodes
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  ## @param dataCoord.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for data coordinator nodes
  ##
  extraEnvVarsCM: ""
  ## @param dataCoord.extraEnvVarsSecret Name of existing Secret containing extra env vars for data coordinator nodes
  ##
  extraEnvVarsSecret: ""
  ## @param dataCoord.defaultConfig [string] Default override configuration from the common set in milvus.defaultConfig
  ##
  defaultConfig: |
    # Override the port for internal binding (the external components will use the service port defined in milvus.defaultConfig)
    dataCoord:
      port: {{ .Values.dataCoord.containerPorts.grpc }}
      enableActiveStandby: true
  ## @param dataCoord.existingConfigMap name of a ConfigMap with existing configuration for the default configuration
  ##
  existingConfigMap: ""
  ## @param dataCoord.extraConfig Override configuration
  ##
  extraConfig: {}
  ## @param dataCoord.extraConfigExistingConfigMap name of a ConfigMap with existing configuration for the Dashboard
  ##
  extraConfigExistingConfigMap: ""
  ## @param dataCoord.command Override default container command (useful when using custom images)
  ##
  command: []
  ## @param dataCoord.args Override default container args (useful when using custom images)
  ##
  args: []
  ## @param dataCoord.replicaCount Number of Data Coordinator replicas to deploy
  ##
  replicaCount: 1
  ## @param dataCoord.containerPorts.grpc GRPC port for Data Coordinator
  ## @param dataCoord.containerPorts.metrics Metrics port for Data Coordinator
  containerPorts:
    grpc: 19530
    metrics: 9091
  ## Configure extra options for Data Coordinator containers' liveness, readiness and startup probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
  ## @param dataCoord.livenessProbe.enabled Enable livenessProbe on Data Coordinator nodes
  ## @param dataCoord.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param dataCoord.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param dataCoord.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param dataCoord.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param dataCoord.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param dataCoord.readinessProbe.enabled Enable readinessProbe on Data Coordinator nodes
  ## @param dataCoord.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param dataCoord.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param dataCoord.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param dataCoord.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param dataCoord.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param dataCoord.startupProbe.enabled Enable startupProbe on Data Coordinator containers
  ## @param dataCoord.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param dataCoord.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param dataCoord.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param dataCoord.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param dataCoord.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param dataCoord.customLivenessProbe Custom livenessProbe that overrides the default one
  ##
  customLivenessProbe: {}
  ## @param dataCoord.customReadinessProbe Custom readinessProbe that overrides the default one
  ##
  customReadinessProbe: {}
  ## @param dataCoord.customStartupProbe Custom startupProbe that overrides the default one
  ##
  customStartupProbe: {}
  ## data coordinator resource requests and limits
  ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param dataCoord.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if dataCoord.resources is set (dataCoord.resources is recommended for production).
  ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
  ##
  resourcesPreset: "micro"
  ## @param dataCoord.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param dataCoord.podSecurityContext.enabled Enabled Data Coordinator pods' Security Context
  ## @param dataCoord.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
  ## @param dataCoord.podSecurityContext.sysctls Set kernel settings using the sysctl interface
  ## @param dataCoord.podSecurityContext.supplementalGroups Set filesystem extra groups
  ## @param dataCoord.podSecurityContext.fsGroup Set Data Coordinator pod's Security Context fsGroup
  ##
  podSecurityContext:
    enabled: true
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## Configure Container Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param dataCoord.containerSecurityContext.enabled Enabled containers' Security Context
  ## @param dataCoord.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
  ## @param dataCoord.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
  ## @param dataCoord.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
  ## @param dataCoord.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
  ## @param dataCoord.containerSecurityContext.privileged Set container's Security Context privileged
  ## @param dataCoord.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
  ## @param dataCoord.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
  ## @param dataCoord.containerSecurityContext.capabilities.drop List of capabilities to be dropped
  ## @param dataCoord.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
  ##
  containerSecurityContext:
    enabled: true
    seLinuxOptions: {}
    runAsUser: 1001
    runAsGroup: 1001
    runAsNonRoot: true
    privileged: false
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    capabilities:
      drop: ["ALL"]
    seccompProfile:
      type: "RuntimeDefault"
  ## @param dataCoord.lifecycleHooks for the data coordinator container(s) to automate configuration before or after startup
  ##
  lifecycleHooks: {}
  ## @param dataCoord.runtimeClassName Name of the runtime class to be used by pod(s)
  ## ref: https://kubernetes.io/docs/concepts/containers/runtime-class/
  ##
  runtimeClassName: ""
  ## @param dataCoord.automountServiceAccountToken Mount Service Account token in pod
  ##
  automountServiceAccountToken: false
  ## @param dataCoord.hostAliases data coordinator pods host aliases
  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
  ##
  hostAliases: []
  ## @param dataCoord.podLabels Extra labels for data coordinator pods
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  ##
  podLabels: {}
  ## @param dataCoord.podAnnotations Annotations for data coordinator pods
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  podAnnotations: {}
  ## @param dataCoord.podAffinityPreset Pod affinity preset. Ignored if `data coordinator.affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAffinityPreset: ""
  ## @param dataCoord.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `data coordinator.affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAntiAffinityPreset: soft
  ## Node data coordinator.affinity preset
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  ##
  nodeAffinityPreset:
    ## @param dataCoord.nodeAffinityPreset.type Node affinity preset type. Ignored if `data coordinator.affinity` is set. Allowed values: `soft` or `hard`
    ##
    type: ""
    ## @param dataCoord.nodeAffinityPreset.key Node label key to match. Ignored if `data coordinator.affinity` is set
    ##
    key: ""
    ## @param dataCoord.nodeAffinityPreset.values Node label values to match. Ignored if `data coordinator.affinity` is set
    ## E.g.
    ## values:
    ##   - e2e-az1
    ##   - e2e-az2
    ##
    values: []
  ## @param dataCoord.affinity Affinity for Data Coordinator pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## NOTE: `dataCoord.podAffinityPreset`, `dataCoord.podAntiAffinityPreset`, and `dataCoord.nodeAffinityPreset` will be ignored when it's set
  ##
  affinity: {}
  ## @param dataCoord.nodeSelector Node labels for Data Coordinator pods assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  ##
  nodeSelector: {}
  ## @param dataCoord.tolerations Tolerations for Data Coordinator pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param dataCoord.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
  ##
  topologySpreadConstraints: []
  ## @param dataCoord.priorityClassName Data Coordinator pods' priorityClassName
  ##
  priorityClassName: ""
  ## @param dataCoord.schedulerName Kubernetes pod scheduler registry
  ## https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""
  ## @param dataCoord.updateStrategy.type Data Coordinator statefulset strategy type
  ## @param dataCoord.updateStrategy.rollingUpdate Data Coordinator statefulset rolling update configuration parameters
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ##
  updateStrategy:
    type: RollingUpdate
    rollingUpdate: {}
  ## @param dataCoord.extraVolumes Optionally specify extra list of additional volumes for the Data Coordinator pod(s)
  ##
  extraVolumes: []
  ## @param dataCoord.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the Data Coordinator container(s)
  ##
  extraVolumeMounts: []
  ## @param dataCoord.sidecars Add additional sidecar containers to the Data Coordinator pod(s)
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## @param dataCoord.enableDefaultInitContainers Deploy default init containers
  ##
  enableDefaultInitContainers: true
  ## @param dataCoord.initContainers Add additional init containers to the Data Coordinator pod(s)
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
  ## e.g:
  ## initContainers:
  ##  - name: your-image-name
  ##    image: your-image
  ##    imagePullPolicy: Always
  ##    command: ['sh', '-c', 'echo "hello world"']
  ##
  initContainers: []
  ## Service account for Data Coordinator to use
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param dataCoord.serviceAccount.create Enable creation of ServiceAccount for Data Coordinator pods
    ##
    create: true
    ## @param dataCoord.serviceAccount.name The name of the ServiceAccount to use
    ## If not set and create is true, a name is generated using the common.names.fullname template
    ##
    name: ""
    ## @param dataCoord.serviceAccount.automountServiceAccountToken Allows auto mount of ServiceAccountToken on the serviceAccount created
    ## Can be set to false if pods using this serviceAccount do not need to use K8s API
    ##
    automountServiceAccountToken: false
    ## @param dataCoord.serviceAccount.annotations Additional custom annotations for the ServiceAccount
    ##
    annotations: {}
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
  ## @param dataCoord.pdb.create Enable/disable a Pod Disruption Budget creation
  ## @param dataCoord.pdb.minAvailable [object] Minimum number/percentage of pods that should remain scheduled
  ## @param dataCoord.pdb.maxUnavailable [object] Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `dataCoord.pdb.minAvailable` and `dataCoord.pdb.maxUnavailable` are empty.
  ##
  pdb:
    create: true
    minAvailable: ""
    maxUnavailable: ""
  ## @section Data Coordinator Autoscaling configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ##
  autoscaling:
    vpa:
      ## @param dataCoord.autoscaling.vpa.enabled Enable VPA
      ##
      enabled: false
      ## @param dataCoord.autoscaling.vpa.annotations Annotations for VPA resource
      ##
      annotations: {}
      ## @param dataCoord.autoscaling.vpa.controlledResources VPA List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
      ##
      controlledResources: []
      ## @param dataCoord.autoscaling.vpa.maxAllowed VPA Max allowed resources for the pod
      ## cpu: 200m
      ## memory: 100Mi
      maxAllowed: {}
      ## @param dataCoord.autoscaling.vpa.minAllowed VPA Min allowed resources for the pod
      ## cpu: 200m
      ## memory: 100Mi
      minAllowed: {}
      updatePolicy:
        ## @param dataCoord.autoscaling.vpa.updatePolicy.updateMode Autoscaling update policy Specifies whether recommended updates are applied when a Pod is started and whether recommended updates are applied during the life of a Pod
        ## Possible values are "Off", "Initial", "Recreate", and "Auto".
        ##
        updateMode: Auto
    hpa:
      ## @param dataCoord.autoscaling.hpa.enabled Enable HPA for Milvus Data Plane
      ##
      enabled: false
      ## @param dataCoord.autoscaling.hpa.annotations Annotations for HPA resource
      ##
      annotations: {}
      ## @param dataCoord.autoscaling.hpa.minReplicas Minimum number of Milvus Data Plane replicas
      ##
      minReplicas: ""
      ## @param dataCoord.autoscaling.hpa.maxReplicas Maximum number of Milvus Data Plane replicas
      ##
      maxReplicas: ""
      ## @param dataCoord.autoscaling.hpa.targetCPU Target CPU utilization percentage
      ##
      targetCPU: ""
      ## @param dataCoord.autoscaling.hpa.targetMemory Target Memory utilization percentage
      ##
      targetMemory: ""
  ## @section Data Coordinator Traffic Exposure Parameters
  ##

  ## data coordinator service parameters
  ##
  service:
    ## @param dataCoord.service.type Data Coordinator service type
    ##
    type: ClusterIP
    ## @param dataCoord.service.ports.grpc Data Coordinator GRPC service port
    ## @param dataCoord.service.ports.metrics Data Coordinator Metrics service port
    ##
    ports:
      grpc: 19530
      metrics: 9091
    ## Node ports to expose
    ## NOTE: choose port between <30000-32767>
    ## @param dataCoord.service.nodePorts.grpc Node port for GRPC
    ## @param dataCoord.service.nodePorts.metrics Node port for Metrics
    ##
    nodePorts:
      grpc: ""
      metrics: ""
    ## @param dataCoord.service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}
    ## @param dataCoord.service.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param dataCoord.service.clusterIP Data Coordinator service Cluster IP
    ## e.g.:
    ## clusterIP: None
    ##
    clusterIP: ""
    ## @param dataCoord.service.loadBalancerIP Data Coordinator service Load Balancer IP
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
    ##
    loadBalancerIP: ""
    ## @param dataCoord.service.loadBalancerSourceRanges Data Coordinator service Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## e.g:
    ## loadBalancerSourceRanges:
    ##   - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## @param dataCoord.service.externalTrafficPolicy Data Coordinator service external traffic policy
    ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param dataCoord.service.annotations Additional custom annotations for Data Coordinator service
    ##
    annotations: {}
    ## @param dataCoord.service.extraPorts Extra ports to expose in the Data Coordinator service
    ##
    extraPorts: []
  ## Network Policy configuration
  ## ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param dataCoord.networkPolicy.enabled Enable creation of NetworkPolicy resources
    ##
    enabled: true
    ## @param dataCoord.networkPolicy.allowExternal The Policy model to apply
    ## When set to false, only pods with the correct client label will have network access to the ports Milvus is
    ## listening on. When true, Milvus will accept connections from any source (with the correct destination port).
    ##
    allowExternal: true
    ## @param dataCoord.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param dataCoord.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraIngress: []
    ## @param dataCoord.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param dataCoord.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces
    ## @param dataCoord.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces
    ##
    ingressNSMatchLabels: {}
    ingressNSPodMatchLabels: {}
  ## @section Data Coordinator Metrics Parameters
  ## Prometheus Exporter / Metrics
  ##
  metrics:
    ## @param dataCoord.metrics.enabled Enable metrics
    ##
    enabled: false
    ## @param dataCoord.metrics.annotations [object] Annotations for the server service in order to scrape metrics
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "{{ .Values.dataCoord.service.ports.grpc }}"
    ## Prometheus Operator ServiceMonitor configuration
    ##
    serviceMonitor:
      ## @param dataCoord.metrics.serviceMonitor.enabled Create ServiceMonitor Resource for scraping metrics using Prometheus Operator
      ##
      enabled: false
      ## @param dataCoord.metrics.serviceMonitor.annotations Annotations for the ServiceMonitor Resource
      ##
      annotations: ""
      ## @param dataCoord.metrics.serviceMonitor.namespace Namespace for the ServiceMonitor Resource (defaults to the Release Namespace)
      ##
      namespace: ""
      ## @param dataCoord.metrics.serviceMonitor.interval Interval at which metrics should be scraped.
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      interval: ""
      ## @param dataCoord.metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      scrapeTimeout: ""
      ## @param dataCoord.metrics.serviceMonitor.labels Additional labels that can be used so ServiceMonitor will be discovered by Prometheus
      ##
      labels: {}
      ## @param dataCoord.metrics.serviceMonitor.selector Prometheus instance selector labels
      ## ref: https://github.com/bitnami/charts/tree/main/bitnami/prometheus-operator#prometheus-configuration
      ##
      selector: {}
      ## @param dataCoord.metrics.serviceMonitor.relabelings RelabelConfigs to apply to samples before scraping
      ##
      relabelings: []
      ## @param dataCoord.metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion
      ##
      metricRelabelings: []
      ## @param dataCoord.metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint
      ##
      honorLabels: false
      ## @param dataCoord.metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in prometheus.
      ##
      jobLabel: ""
## @section Root Coordinator Deployment Parameters
##
rootCoord:
  ## @param rootCoord.enabled Enable Root Coordinator deployment
  ##
  enabled: true
  ## @param rootCoord.extraEnvVars Array with extra environment variables to add to data coordinator nodes
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  ## @param rootCoord.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for data coordinator nodes
  ##
  extraEnvVarsCM: ""
  ## @param rootCoord.extraEnvVarsSecret Name of existing Secret containing extra env vars for data coordinator nodes
  ##
  extraEnvVarsSecret: ""
  ## @param rootCoord.defaultConfig [string] Default override configuration from the common set in milvus.defaultConfig
  ##
  defaultConfig: |
    # Override the port for internal binding (the external components will use the service port defined in milvus.defaultConfig)
    rootCoord:
      port: {{ .Values.rootCoord.containerPorts.grpc }}
      enableActiveStandby: true
  ## @param rootCoord.existingConfigMap name of a ConfigMap with existing configuration for the default configuration
  ##
  existingConfigMap: ""
  ## @param rootCoord.extraConfig Override configuration
  ##
  extraConfig: {}
  ## @param rootCoord.extraConfigExistingConfigMap name of a ConfigMap with existing configuration for the Dashboard
  ##
  extraConfigExistingConfigMap: ""
  ## @param rootCoord.command Override default container command (useful when using custom images)
  ##
  command: []
  ## @param rootCoord.args Override default container args (useful when using custom images)
  ##
  args: []
  ## @param rootCoord.replicaCount Number of Root Coordinator replicas to deploy
  ##
  replicaCount: 1
  ## @param rootCoord.containerPorts.grpc GRPC port for Root Coordinator
  ## @param rootCoord.containerPorts.metrics Metrics port for Root Coordinator
  containerPorts:
    grpc: 19530
    metrics: 9091
  ## Configure extra options for Root Coordinator containers' liveness, readiness and startup probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
  ## @param rootCoord.livenessProbe.enabled Enable livenessProbe on Root Coordinator nodes
  ## @param rootCoord.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param rootCoord.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param rootCoord.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param rootCoord.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param rootCoord.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param rootCoord.readinessProbe.enabled Enable readinessProbe on Root Coordinator nodes
  ## @param rootCoord.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param rootCoord.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param rootCoord.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param rootCoord.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param rootCoord.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param rootCoord.startupProbe.enabled Enable startupProbe on Root Coordinator containers
  ## @param rootCoord.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param rootCoord.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param rootCoord.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param rootCoord.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param rootCoord.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param rootCoord.customLivenessProbe Custom livenessProbe that overrides the default one
  ##
  customLivenessProbe: {}
  ## @param rootCoord.customReadinessProbe Custom readinessProbe that overrides the default one
  ##
  customReadinessProbe: {}
  ## @param rootCoord.customStartupProbe Custom startupProbe that overrides the default one
  ##
  customStartupProbe: {}
  ## data coordinator resource requests and limits
  ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param rootCoord.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if rootCoord.resources is set (rootCoord.resources is recommended for production).
  ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
  ##
  resourcesPreset: "micro"
  ## @param rootCoord.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param rootCoord.podSecurityContext.enabled Enabled Root Coordinator pods' Security Context
  ## @param rootCoord.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
  ## @param rootCoord.podSecurityContext.sysctls Set kernel settings using the sysctl interface
  ## @param rootCoord.podSecurityContext.supplementalGroups Set filesystem extra groups
  ## @param rootCoord.podSecurityContext.fsGroup Set Root Coordinator pod's Security Context fsGroup
  ##
  podSecurityContext:
    enabled: true
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## Configure Container Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param rootCoord.containerSecurityContext.enabled Enabled containers' Security Context
  ## @param rootCoord.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
  ## @param rootCoord.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
  ## @param rootCoord.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
  ## @param rootCoord.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
  ## @param rootCoord.containerSecurityContext.privileged Set container's Security Context privileged
  ## @param rootCoord.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
  ## @param rootCoord.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
  ## @param rootCoord.containerSecurityContext.capabilities.drop List of capabilities to be dropped
  ## @param rootCoord.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
  ##
  containerSecurityContext:
    enabled: true
    seLinuxOptions: {}
    runAsUser: 1001
    runAsGroup: 1001
    runAsNonRoot: true
    privileged: false
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    capabilities:
      drop: ["ALL"]
    seccompProfile:
      type: "RuntimeDefault"
  ## @param rootCoord.lifecycleHooks for the data coordinator container(s) to automate configuration before or after startup
  ##
  lifecycleHooks: {}
  ## @param rootCoord.runtimeClassName Name of the runtime class to be used by pod(s)
  ## ref: https://kubernetes.io/docs/concepts/containers/runtime-class/
  ##
  runtimeClassName: ""
  ## @param rootCoord.automountServiceAccountToken Mount Service Account token in pod
  ##
  automountServiceAccountToken: false
  ## @param rootCoord.hostAliases data coordinator pods host aliases
  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
  ##
  hostAliases: []
  ## @param rootCoord.podLabels Extra labels for data coordinator pods
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  ##
  podLabels: {}
  ## @param rootCoord.podAnnotations Annotations for data coordinator pods
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  podAnnotations: {}
  ## @param rootCoord.podAffinityPreset Pod affinity preset. Ignored if `data coordinator.affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAffinityPreset: ""
  ## @param rootCoord.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `data coordinator.affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAntiAffinityPreset: soft
  ## Node data coordinator.affinity preset
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  ##
  nodeAffinityPreset:
    ## @param rootCoord.nodeAffinityPreset.type Node affinity preset type. Ignored if `data coordinator.affinity` is set. Allowed values: `soft` or `hard`
    ##
    type: ""
    ## @param rootCoord.nodeAffinityPreset.key Node label key to match. Ignored if `data coordinator.affinity` is set
    ##
    key: ""
    ## @param rootCoord.nodeAffinityPreset.values Node label values to match. Ignored if `data coordinator.affinity` is set
    ## E.g.
    ## values:
    ##   - e2e-az1
    ##   - e2e-az2
    ##
    values: []
  ## @param rootCoord.affinity Affinity for Root Coordinator pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## NOTE: `rootCoord.podAffinityPreset`, `rootCoord.podAntiAffinityPreset`, and `rootCoord.nodeAffinityPreset` will be ignored when it's set
  ##
  affinity: {}
  ## @param rootCoord.nodeSelector Node labels for Root Coordinator pods assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  ##
  nodeSelector: {}
  ## @param rootCoord.tolerations Tolerations for Root Coordinator pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param rootCoord.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
  ##
  topologySpreadConstraints: []
  ## @param rootCoord.priorityClassName Root Coordinator pods' priorityClassName
  ##
  priorityClassName: ""
  ## @param rootCoord.schedulerName Kubernetes pod scheduler registry
  ## https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""
  ## @param rootCoord.updateStrategy.type Root Coordinator statefulset strategy type
  ## @param rootCoord.updateStrategy.rollingUpdate Root Coordinator statefulset rolling update configuration parameters
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ##
  updateStrategy:
    type: RollingUpdate
    rollingUpdate: {}
  ## @param rootCoord.extraVolumes Optionally specify extra list of additional volumes for the Root Coordinator pod(s)
  ##
  extraVolumes: []
  ## @param rootCoord.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the Root Coordinator container(s)
  ##
  extraVolumeMounts: []
  ## @param rootCoord.sidecars Add additional sidecar containers to the Root Coordinator pod(s)
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## @param rootCoord.enableDefaultInitContainers Deploy default init containers
  ##
  enableDefaultInitContainers: true
  ## @param rootCoord.initContainers Add additional init containers to the Root Coordinator pod(s)
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
  ## e.g:
  ## initContainers:
  ##  - name: your-image-name
  ##    image: your-image
  ##    imagePullPolicy: Always
  ##    command: ['sh', '-c', 'echo "hello world"']
  ##
  initContainers: []
  ## Service account for Root Coordinator to use
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param rootCoord.serviceAccount.create Enable creation of ServiceAccount for Root Coordinator pods
    ##
    create: true
    ## @param rootCoord.serviceAccount.name The name of the ServiceAccount to use
    ## If not set and create is true, a name is generated using the common.names.fullname template
    ##
    name: ""
    ## @param rootCoord.serviceAccount.automountServiceAccountToken Allows auto mount of ServiceAccountToken on the serviceAccount created
    ## Can be set to false if pods using this serviceAccount do not need to use K8s API
    ##
    automountServiceAccountToken: false
    ## @param rootCoord.serviceAccount.annotations Additional custom annotations for the ServiceAccount
    ##
    annotations: {}
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
  ## @param rootCoord.pdb.create Enable/disable a Pod Disruption Budget creation
  ## @param rootCoord.pdb.minAvailable [object] Minimum number/percentage of pods that should remain scheduled
  ## @param rootCoord.pdb.maxUnavailable [object] Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `rootCoord.pdb.minAvailable` and `rootCoord.pdb.maxUnavailable` are empty.
  ##
  pdb:
    create: true
    minAvailable: ""
    maxUnavailable: ""
  ## @section Root Coordinator Autoscaling configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ##
  autoscaling:
    vpa:
      ## @param rootCoord.autoscaling.vpa.enabled Enable VPA
      ##
      enabled: false
      ## @param rootCoord.autoscaling.vpa.annotations Annotations for VPA resource
      ##
      annotations: {}
      ## @param rootCoord.autoscaling.vpa.controlledResources VPA List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
      ##
      controlledResources: []
      ## @param rootCoord.autoscaling.vpa.maxAllowed VPA Max allowed resources for the pod
      ## cpu: 200m
      ## memory: 100Mi
      maxAllowed: {}
      ## @param rootCoord.autoscaling.vpa.minAllowed VPA Min allowed resources for the pod
      ## cpu: 200m
      ## memory: 100Mi
      minAllowed: {}
      updatePolicy:
        ## @param rootCoord.autoscaling.vpa.updatePolicy.updateMode Autoscaling update policy Specifies whether recommended updates are applied when a Pod is started and whether recommended updates are applied during the life of a Pod
        ## Possible values are "Off", "Initial", "Recreate", and "Auto".
        ##
        updateMode: Auto
    hpa:
      ## @param rootCoord.autoscaling.hpa.enabled Enable HPA for Milvus Data Plane
      ##
      enabled: false
      ## @param rootCoord.autoscaling.hpa.annotations Annotations for HPA resource
      ##
      annotations: {}
      ## @param rootCoord.autoscaling.hpa.minReplicas Minimum number of Milvus Data Plane replicas
      ##
      minReplicas: ""
      ## @param rootCoord.autoscaling.hpa.maxReplicas Maximum number of Milvus Data Plane replicas
      ##
      maxReplicas: ""
      ## @param rootCoord.autoscaling.hpa.targetCPU Target CPU utilization percentage
      ##
      targetCPU: ""
      ## @param rootCoord.autoscaling.hpa.targetMemory Target Memory utilization percentage
      ##
      targetMemory: ""
  ## @section Root Coordinator Traffic Exposure Parameters
  ##

  ## data coordinator service parameters
  ##
  service:
    ## @param rootCoord.service.type Root Coordinator service type
    ##
    type: ClusterIP
    ## @param rootCoord.service.ports.grpc Root Coordinator GRPC service port
    ## @param rootCoord.service.ports.metrics Root Coordinator Metrics service port
    ##
    ports:
      grpc: 19530
      metrics: 9091
    ## Node ports to expose
    ## NOTE: choose port between <30000-32767>
    ## @param rootCoord.service.nodePorts.grpc Node port for GRPC
    ## @param rootCoord.service.nodePorts.metrics Node port for Metrics
    ##
    nodePorts:
      grpc: ""
      metrics: ""
    ## @param rootCoord.service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}
    ## @param rootCoord.service.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param rootCoord.service.clusterIP Root Coordinator service Cluster IP
    ## e.g.:
    ## clusterIP: None
    ##
    clusterIP: ""
    ## @param rootCoord.service.loadBalancerIP Root Coordinator service Load Balancer IP
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
    ##
    loadBalancerIP: ""
    ## @param rootCoord.service.loadBalancerSourceRanges Root Coordinator service Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## e.g:
    ## loadBalancerSourceRanges:
    ##   - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## @param rootCoord.service.externalTrafficPolicy Root Coordinator service external traffic policy
    ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param rootCoord.service.annotations Additional custom annotations for Root Coordinator service
    ##
    annotations: {}
    ## @param rootCoord.service.extraPorts Extra ports to expose in the Root Coordinator service
    ##
    extraPorts: []
  ## Network Policy configuration
  ## ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param rootCoord.networkPolicy.enabled Enable creation of NetworkPolicy resources
    ##
    enabled: true
    ## @param rootCoord.networkPolicy.allowExternal The Policy model to apply
    ## When set to false, only pods with the correct client label will have network access to the ports Milvus is
    ## listening on. When true, Milvus will accept connections from any source (with the correct destination port).
    ##
    allowExternal: true
    ## @param rootCoord.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param rootCoord.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraIngress: []
    ## @param rootCoord.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param rootCoord.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces
    ## @param rootCoord.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces
    ##
    ingressNSMatchLabels: {}
    ingressNSPodMatchLabels: {}
  ## @section Root Coordinator Metrics Parameters
  ## Prometheus Exporter / Metrics
  ##
  metrics:
    ## @param rootCoord.metrics.enabled Enable metrics
    ##
    enabled: false
    ## @param rootCoord.metrics.annotations [object] Annotations for the server service in order to scrape metrics
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "{{ .Values.rootCoord.service.ports.grpc }}"
    ## Prometheus Operator ServiceMonitor configuration
    ##
    serviceMonitor:
      ## @param rootCoord.metrics.serviceMonitor.enabled Create ServiceMonitor Resource for scraping metrics using Prometheus Operator
      ##
      enabled: false
      ## @param rootCoord.metrics.serviceMonitor.annotations Annotations for the ServiceMonitor Resource
      ##
      annotations: ""
      ## @param rootCoord.metrics.serviceMonitor.namespace Namespace for the ServiceMonitor Resource (defaults to the Release Namespace)
      ##
      namespace: ""
      ## @param rootCoord.metrics.serviceMonitor.interval Interval at which metrics should be scraped.
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      interval: ""
      ## @param rootCoord.metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      scrapeTimeout: ""
      ## @param rootCoord.metrics.serviceMonitor.labels Additional labels that can be used so ServiceMonitor will be discovered by Prometheus
      ##
      labels: {}
      ## @param rootCoord.metrics.serviceMonitor.selector Prometheus instance selector labels
      ## ref: https://github.com/bitnami/charts/tree/main/bitnami/prometheus-operator#prometheus-configuration
      ##
      selector: {}
      ## @param rootCoord.metrics.serviceMonitor.relabelings RelabelConfigs to apply to samples before scraping
      ##
      relabelings: []
      ## @param rootCoord.metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion
      ##
      metricRelabelings: []
      ## @param rootCoord.metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint
      ##
      honorLabels: false
      ## @param rootCoord.metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in prometheus.
      ##
      jobLabel: ""
## @section Query Coordinator Deployment Parameters
##
queryCoord:
  ## @param queryCoord.enabled Enable Query Coordinator deployment
  ##
  enabled: true
  ## @param queryCoord.extraEnvVars Array with extra environment variables to add to data coordinator nodes
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  ## @param queryCoord.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for data coordinator nodes
  ##
  extraEnvVarsCM: ""
  ## @param queryCoord.extraEnvVarsSecret Name of existing Secret containing extra env vars for data coordinator nodes
  ##
  extraEnvVarsSecret: ""
  ## @param queryCoord.defaultConfig [string] Default override configuration from the common set in milvus.defaultConfig
  ##
  defaultConfig: |
    # Override the port for internal binding (the external components will use the service port defined in milvus.defaultConfig)
    queryCoord:
      port: {{ .Values.queryCoord.containerPorts.grpc }}
      enableActiveStandby: true
  ## @param queryCoord.existingConfigMap name of a ConfigMap with existing configuration for the default configuration
  ##
  existingConfigMap: ""
  ## @param queryCoord.extraConfig Override configuration
  ##
  extraConfig: {}
  ## @param queryCoord.extraConfigExistingConfigMap name of a ConfigMap with existing configuration for the Dashboard
  ##
  extraConfigExistingConfigMap: ""
  ## @param queryCoord.command Override default container command (useful when using custom images)
  ##
  command: []
  ## @param queryCoord.args Override default container args (useful when using custom images)
  ##
  args: []
  ## @param queryCoord.replicaCount Number of Query Coordinator replicas to deploy
  ##
  replicaCount: 1
  ## @param queryCoord.containerPorts.grpc GRPC port for Query Coordinator
  ## @param queryCoord.containerPorts.metrics Metrics port for Query Coordinator
  containerPorts:
    grpc: 19530
    metrics: 9091
  ## Configure extra options for Query Coordinator containers' liveness, readiness and startup probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
  ## @param queryCoord.livenessProbe.enabled Enable livenessProbe on Query Coordinator nodes
  ## @param queryCoord.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param queryCoord.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param queryCoord.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param queryCoord.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param queryCoord.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param queryCoord.readinessProbe.enabled Enable readinessProbe on Query Coordinator nodes
  ## @param queryCoord.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param queryCoord.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param queryCoord.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param queryCoord.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param queryCoord.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param queryCoord.startupProbe.enabled Enable startupProbe on Query Coordinator containers
  ## @param queryCoord.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param queryCoord.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param queryCoord.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param queryCoord.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param queryCoord.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param queryCoord.customLivenessProbe Custom livenessProbe that overrides the default one
  ##
  customLivenessProbe: {}
  ## @param queryCoord.customReadinessProbe Custom readinessProbe that overrides the default one
  ##
  customReadinessProbe: {}
  ## @param queryCoord.customStartupProbe Custom startupProbe that overrides the default one
  ##
  customStartupProbe: {}
  ## data coordinator resource requests and limits
  ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param queryCoord.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if queryCoord.resources is set (queryCoord.resources is recommended for production).
  ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
  ##
  resourcesPreset: "micro"
  ## @param queryCoord.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param queryCoord.podSecurityContext.enabled Enabled Query Coordinator pods' Security Context
  ## @param queryCoord.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
  ## @param queryCoord.podSecurityContext.sysctls Set kernel settings using the sysctl interface
  ## @param queryCoord.podSecurityContext.supplementalGroups Set filesystem extra groups
  ## @param queryCoord.podSecurityContext.fsGroup Set Query Coordinator pod's Security Context fsGroup
  ##
  podSecurityContext:
    enabled: true
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## Configure Container Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param queryCoord.containerSecurityContext.enabled Enabled containers' Security Context
  ## @param queryCoord.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
  ## @param queryCoord.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
  ## @param queryCoord.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
  ## @param queryCoord.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
  ## @param queryCoord.containerSecurityContext.privileged Set container's Security Context privileged
  ## @param queryCoord.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
  ## @param queryCoord.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
  ## @param queryCoord.containerSecurityContext.capabilities.drop List of capabilities to be dropped
  ## @param queryCoord.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
  ##
  containerSecurityContext:
    enabled: true
    seLinuxOptions: {}
    runAsUser: 1001
    runAsGroup: 1001
    runAsNonRoot: true
    privileged: false
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    capabilities:
      drop: ["ALL"]
    seccompProfile:
      type: "RuntimeDefault"
  ## @param queryCoord.lifecycleHooks for the data coordinator container(s) to automate configuration before or after startup
  ##
  lifecycleHooks: {}
  ## @param queryCoord.runtimeClassName Name of the runtime class to be used by pod(s)
  ## ref: https://kubernetes.io/docs/concepts/containers/runtime-class/
  ##
  runtimeClassName: ""
  ## @param queryCoord.automountServiceAccountToken Mount Service Account token in pod
  ##
  automountServiceAccountToken: false
  ## @param queryCoord.hostAliases data coordinator pods host aliases
  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
  ##
  hostAliases: []
  ## @param queryCoord.podLabels Extra labels for data coordinator pods
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  ##
  podLabels: {}
  ## @param queryCoord.podAnnotations Annotations for data coordinator pods
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  podAnnotations: {}
  ## @param queryCoord.podAffinityPreset Pod affinity preset. Ignored if `data coordinator.affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAffinityPreset: ""
  ## @param queryCoord.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `data coordinator.affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAntiAffinityPreset: soft
  ## Node data coordinator.affinity preset
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  ##
  nodeAffinityPreset:
    ## @param queryCoord.nodeAffinityPreset.type Node affinity preset type. Ignored if `data coordinator.affinity` is set. Allowed values: `soft` or `hard`
    ##
    type: ""
    ## @param queryCoord.nodeAffinityPreset.key Node label key to match. Ignored if `data coordinator.affinity` is set
    ##
    key: ""
    ## @param queryCoord.nodeAffinityPreset.values Node label values to match. Ignored if `data coordinator.affinity` is set
    ## E.g.
    ## values:
    ##   - e2e-az1
    ##   - e2e-az2
    ##
    values: []
  ## @param queryCoord.affinity Affinity for Query Coordinator pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## NOTE: `queryCoord.podAffinityPreset`, `queryCoord.podAntiAffinityPreset`, and `queryCoord.nodeAffinityPreset` will be ignored when it's set
  ##
  affinity: {}
  ## @param queryCoord.nodeSelector Node labels for Query Coordinator pods assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  ##
  nodeSelector: {}
  ## @param queryCoord.tolerations Tolerations for Query Coordinator pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param queryCoord.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
  ##
  topologySpreadConstraints: []
  ## @param queryCoord.priorityClassName Query Coordinator pods' priorityClassName
  ##
  priorityClassName: ""
  ## @param queryCoord.schedulerName Kubernetes pod scheduler registry
  ## https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""
  ## @param queryCoord.updateStrategy.type Query Coordinator statefulset strategy type
  ## @param queryCoord.updateStrategy.rollingUpdate Query Coordinator statefulset rolling update configuration parameters
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ##
  updateStrategy:
    type: RollingUpdate
    rollingUpdate: {}
  ## @param queryCoord.extraVolumes Optionally specify extra list of additional volumes for the Query Coordinator pod(s)
  ##
  extraVolumes: []
  ## @param queryCoord.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the Query Coordinator container(s)
  ##
  extraVolumeMounts: []
  ## @param queryCoord.sidecars Add additional sidecar containers to the Query Coordinator pod(s)
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## @param queryCoord.enableDefaultInitContainers Deploy default init containers
  ##
  enableDefaultInitContainers: true
  ## @param queryCoord.initContainers Add additional init containers to the Query Coordinator pod(s)
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
  ## e.g:
  ## initContainers:
  ##  - name: your-image-name
  ##    image: your-image
  ##    imagePullPolicy: Always
  ##    command: ['sh', '-c', 'echo "hello world"']
  ##
  initContainers: []
  ## Service account for Query Coordinator to use
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param queryCoord.serviceAccount.create Enable creation of ServiceAccount for Query Coordinator pods
    ##
    create: true
    ## @param queryCoord.serviceAccount.name The name of the ServiceAccount to use
    ## If not set and create is true, a name is generated using the common.names.fullname template
    ##
    name: ""
    ## @param queryCoord.serviceAccount.automountServiceAccountToken Allows auto mount of ServiceAccountToken on the serviceAccount created
    ## Can be set to false if pods using this serviceAccount do not need to use K8s API
    ##
    automountServiceAccountToken: false
    ## @param queryCoord.serviceAccount.annotations Additional custom annotations for the ServiceAccount
    ##
    annotations: {}
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
  ## @param queryCoord.pdb.create Enable/disable a Pod Disruption Budget creation
  ## @param queryCoord.pdb.minAvailable [object] Minimum number/percentage of pods that should remain scheduled
  ## @param queryCoord.pdb.maxUnavailable [object] Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `queryCoord.pdb.minAvailable` and `queryCoord.pdb.maxUnavailable` are empty.
  ##
  pdb:
    create: true
    minAvailable: ""
    maxUnavailable: ""
  ## @section Query Coordinator Autoscaling configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ##
  autoscaling:
    vpa:
      ## @param queryCoord.autoscaling.vpa.enabled Enable VPA
      ##
      enabled: false
      ## @param queryCoord.autoscaling.vpa.annotations Annotations for VPA resource
      ##
      annotations: {}
      ## @param queryCoord.autoscaling.vpa.controlledResources VPA List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
      ##
      controlledResources: []
      ## @param queryCoord.autoscaling.vpa.maxAllowed VPA Max allowed resources for the pod
      ## cpu: 200m
      ## memory: 100Mi
      maxAllowed: {}
      ## @param queryCoord.autoscaling.vpa.minAllowed VPA Min allowed resources for the pod
      ## cpu: 200m
      ## memory: 100Mi
      minAllowed: {}
      updatePolicy:
        ## @param queryCoord.autoscaling.vpa.updatePolicy.updateMode Autoscaling update policy Specifies whether recommended updates are applied when a Pod is started and whether recommended updates are applied during the life of a Pod
        ## Possible values are "Off", "Initial", "Recreate", and "Auto".
        ##
        updateMode: Auto
    hpa:
      ## @param queryCoord.autoscaling.hpa.enabled Enable HPA for Milvus Data Plane
      ##
      enabled: false
      ## @param queryCoord.autoscaling.hpa.annotations Annotations for HPA resource
      ##
      annotations: {}
      ## @param queryCoord.autoscaling.hpa.minReplicas Minimum number of Milvus Data Plane replicas
      ##
      minReplicas: ""
      ## @param queryCoord.autoscaling.hpa.maxReplicas Maximum number of Milvus Data Plane replicas
      ##
      maxReplicas: ""
      ## @param queryCoord.autoscaling.hpa.targetCPU Target CPU utilization percentage
      ##
      targetCPU: ""
      ## @param queryCoord.autoscaling.hpa.targetMemory Target Memory utilization percentage
      ##
      targetMemory: ""
  ## @section Query Coordinator Traffic Exposure Parameters
  ##

  ## data coordinator service parameters
  ##
  service:
    ## @param queryCoord.service.type Query Coordinator service type
    ##
    type: ClusterIP
    ## @param queryCoord.service.ports.grpc Query Coordinator GRPC service port
    ## @param queryCoord.service.ports.metrics Query Coordinator Metrics service port
    ##
    ports:
      grpc: 19530
      metrics: 9091
    ## Node ports to expose
    ## NOTE: choose port between <30000-32767>
    ## @param queryCoord.service.nodePorts.grpc Node port for GRPC
    ## @param queryCoord.service.nodePorts.metrics Node port for Metrics
    ##
    nodePorts:
      grpc: ""
      metrics: ""
    ## @param queryCoord.service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}
    ## @param queryCoord.service.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param queryCoord.service.clusterIP Query Coordinator service Cluster IP
    ## e.g.:
    ## clusterIP: None
    ##
    clusterIP: ""
    ## @param queryCoord.service.loadBalancerIP Query Coordinator service Load Balancer IP
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
    ##
    loadBalancerIP: ""
    ## @param queryCoord.service.loadBalancerSourceRanges Query Coordinator service Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## e.g:
    ## loadBalancerSourceRanges:
    ##   - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## @param queryCoord.service.externalTrafficPolicy Query Coordinator service external traffic policy
    ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param queryCoord.service.annotations Additional custom annotations for Query Coordinator service
    ##
    annotations: {}
    ## @param queryCoord.service.extraPorts Extra ports to expose in the Query Coordinator service
    ##
    extraPorts: []
  ## Network Policy configuration
  ## ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param queryCoord.networkPolicy.enabled Enable creation of NetworkPolicy resources
    ##
    enabled: true
    ## @param queryCoord.networkPolicy.allowExternal The Policy model to apply
    ## When set to false, only pods with the correct client label will have network access to the ports Milvus is
    ## listening on. When true, Milvus will accept connections from any source (with the correct destination port).
    ##
    allowExternal: true
    ## @param queryCoord.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param queryCoord.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraIngress: []
    ## @param queryCoord.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param queryCoord.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces
    ## @param queryCoord.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces
    ##
    ingressNSMatchLabels: {}
    ingressNSPodMatchLabels: {}
  ## @section Query Coordinator Metrics Parameters
  ## Prometheus Exporter / Metrics
  ##
  metrics:
    ## @param queryCoord.metrics.enabled Enable metrics
    ##
    enabled: false
    ## @param queryCoord.metrics.annotations [object] Annotations for the server service in order to scrape metrics
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "{{ .Values.queryCoord.service.ports.grpc }}"
    ## Prometheus Operator ServiceMonitor configuration
    ##
    serviceMonitor:
      ## @param queryCoord.metrics.serviceMonitor.enabled Create ServiceMonitor Resource for scraping metrics using Prometheus Operator
      ##
      enabled: false
      ## @param queryCoord.metrics.serviceMonitor.annotations Annotations for the ServiceMonitor Resource
      ##
      annotations: ""
      ## @param queryCoord.metrics.serviceMonitor.namespace Namespace for the ServiceMonitor Resource (defaults to the Release Namespace)
      ##
      namespace: ""
      ## @param queryCoord.metrics.serviceMonitor.interval Interval at which metrics should be scraped.
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      interval: ""
      ## @param queryCoord.metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      scrapeTimeout: ""
      ## @param queryCoord.metrics.serviceMonitor.labels Additional labels that can be used so ServiceMonitor will be discovered by Prometheus
      ##
      labels: {}
      ## @param queryCoord.metrics.serviceMonitor.selector Prometheus instance selector labels
      ## ref: https://github.com/bitnami/charts/tree/main/bitnami/prometheus-operator#prometheus-configuration
      ##
      selector: {}
      ## @param queryCoord.metrics.serviceMonitor.relabelings RelabelConfigs to apply to samples before scraping
      ##
      relabelings: []
      ## @param queryCoord.metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion
      ##
      metricRelabelings: []
      ## @param queryCoord.metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint
      ##
      honorLabels: false
      ## @param queryCoord.metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in prometheus.
      ##
      jobLabel: ""
## @section Index Coordinator Deployment Parameters
##
indexCoord:
  ## @param indexCoord.enabled Enable Index Coordinator deployment
  ##
  enabled: true
  ## @param indexCoord.extraEnvVars Array with extra environment variables to add to data coordinator nodes
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  ## @param indexCoord.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for data coordinator nodes
  ##
  extraEnvVarsCM: ""
  ## @param indexCoord.extraEnvVarsSecret Name of existing Secret containing extra env vars for data coordinator nodes
  ##
  extraEnvVarsSecret: ""
  ## @param indexCoord.defaultConfig [string] Default override configuration from the common set in milvus.defaultConfig
  ##
  defaultConfig: |
    # Override the port for internal binding (the external components will use the service port defined in milvus.defaultConfig)
    indexCoord:
      port: {{ .Values.indexCoord.containerPorts.grpc }}
      enableActiveStandby: true
  ## @param indexCoord.existingConfigMap name of a ConfigMap with existing configuration for the default configuration
  ##
  existingConfigMap: ""
  ## @param indexCoord.extraConfig Override configuration
  ##
  extraConfig: {}
  ## @param indexCoord.extraConfigExistingConfigMap name of a ConfigMap with existing configuration for the Dashboard
  ##
  extraConfigExistingConfigMap: ""
  ## @param indexCoord.command Override default container command (useful when using custom images)
  ##
  command: []
  ## @param indexCoord.args Override default container args (useful when using custom images)
  ##
  args: []
  ## @param indexCoord.replicaCount Number of Index Coordinator replicas to deploy
  ##
  replicaCount: 1
  ## @param indexCoord.containerPorts.grpc GRPC port for Index Coordinator
  ## @param indexCoord.containerPorts.metrics Metrics port for Index Coordinator
  containerPorts:
    grpc: 19530
    metrics: 9091
  ## Configure extra options for Index Coordinator containers' liveness, readiness and startup probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
  ## @param indexCoord.livenessProbe.enabled Enable livenessProbe on Index Coordinator nodes
  ## @param indexCoord.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param indexCoord.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param indexCoord.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param indexCoord.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param indexCoord.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param indexCoord.readinessProbe.enabled Enable readinessProbe on Index Coordinator nodes
  ## @param indexCoord.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param indexCoord.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param indexCoord.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param indexCoord.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param indexCoord.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param indexCoord.startupProbe.enabled Enable startupProbe on Index Coordinator containers
  ## @param indexCoord.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param indexCoord.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param indexCoord.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param indexCoord.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param indexCoord.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param indexCoord.customLivenessProbe Custom livenessProbe that overrides the default one
  ##
  customLivenessProbe: {}
  ## @param indexCoord.customReadinessProbe Custom readinessProbe that overrides the default one
  ##
  customReadinessProbe: {}
  ## @param indexCoord.customStartupProbe Custom startupProbe that overrides the default one
  ##
  customStartupProbe: {}
  ## data coordinator resource requests and limits
  ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param indexCoord.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if indexCoord.resources is set (indexCoord.resources is recommended for production).
  ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
  ##
  resourcesPreset: "micro"
  ## @param indexCoord.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param indexCoord.podSecurityContext.enabled Enabled Index Coordinator pods' Security Context
  ## @param indexCoord.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
  ## @param indexCoord.podSecurityContext.sysctls Set kernel settings using the sysctl interface
  ## @param indexCoord.podSecurityContext.supplementalGroups Set filesystem extra groups
  ## @param indexCoord.podSecurityContext.fsGroup Set Index Coordinator pod's Security Context fsGroup
  ##
  podSecurityContext:
    enabled: true
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## Configure Container Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param indexCoord.containerSecurityContext.enabled Enabled containers' Security Context
  ## @param indexCoord.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
  ## @param indexCoord.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
  ## @param indexCoord.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
  ## @param indexCoord.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
  ## @param indexCoord.containerSecurityContext.privileged Set container's Security Context privileged
  ## @param indexCoord.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
  ## @param indexCoord.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
  ## @param indexCoord.containerSecurityContext.capabilities.drop List of capabilities to be dropped
  ## @param indexCoord.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
  ##
  containerSecurityContext:
    enabled: true
    seLinuxOptions: {}
    runAsUser: 1001
    runAsGroup: 1001
    runAsNonRoot: true
    privileged: false
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    capabilities:
      drop: ["ALL"]
    seccompProfile:
      type: "RuntimeDefault"
  ## @param indexCoord.lifecycleHooks for the data coordinator container(s) to automate configuration before or after startup
  ##
  lifecycleHooks: {}
  ## @param indexCoord.runtimeClassName Name of the runtime class to be used by pod(s)
  ## ref: https://kubernetes.io/docs/concepts/containers/runtime-class/
  ##
  runtimeClassName: ""
  ## @param indexCoord.automountServiceAccountToken Mount Service Account token in pod
  ##
  automountServiceAccountToken: false
  ## @param indexCoord.hostAliases data coordinator pods host aliases
  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
  ##
  hostAliases: []
  ## @param indexCoord.podLabels Extra labels for data coordinator pods
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  ##
  podLabels: {}
  ## @param indexCoord.podAnnotations Annotations for data coordinator pods
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  podAnnotations: {}
  ## @param indexCoord.podAffinityPreset Pod affinity preset. Ignored if `data coordinator.affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAffinityPreset: ""
  ## @param indexCoord.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `data coordinator.affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAntiAffinityPreset: soft
  ## Node data coordinator.affinity preset
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  ##
  nodeAffinityPreset:
    ## @param indexCoord.nodeAffinityPreset.type Node affinity preset type. Ignored if `data coordinator.affinity` is set. Allowed values: `soft` or `hard`
    ##
    type: ""
    ## @param indexCoord.nodeAffinityPreset.key Node label key to match. Ignored if `data coordinator.affinity` is set
    ##
    key: ""
    ## @param indexCoord.nodeAffinityPreset.values Node label values to match. Ignored if `data coordinator.affinity` is set
    ## E.g.
    ## values:
    ##   - e2e-az1
    ##   - e2e-az2
    ##
    values: []
  ## @param indexCoord.affinity Affinity for Index Coordinator pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## NOTE: `indexCoord.podAffinityPreset`, `indexCoord.podAntiAffinityPreset`, and `indexCoord.nodeAffinityPreset` will be ignored when it's set
  ##
  affinity: {}
  ## @param indexCoord.nodeSelector Node labels for Index Coordinator pods assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  ##
  nodeSelector: {}
  ## @param indexCoord.tolerations Tolerations for Index Coordinator pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param indexCoord.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
  ##
  topologySpreadConstraints: []
  ## @param indexCoord.priorityClassName Index Coordinator pods' priorityClassName
  ##
  priorityClassName: ""
  ## @param indexCoord.schedulerName Kubernetes pod scheduler registry
  ## https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""
  ## @param indexCoord.updateStrategy.type Index Coordinator statefulset strategy type
  ## @param indexCoord.updateStrategy.rollingUpdate Index Coordinator statefulset rolling update configuration parameters
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ##
  updateStrategy:
    type: RollingUpdate
    rollingUpdate: {}
  ## @param indexCoord.extraVolumes Optionally specify extra list of additional volumes for the Index Coordinator pod(s)
  ##
  extraVolumes: []
  ## @param indexCoord.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the Index Coordinator container(s)
  ##
  extraVolumeMounts: []
  ## @param indexCoord.sidecars Add additional sidecar containers to the Index Coordinator pod(s)
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## @param indexCoord.enableDefaultInitContainers Deploy default init containers
  ##
  enableDefaultInitContainers: true
  ## @param indexCoord.initContainers Add additional init containers to the Index Coordinator pod(s)
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
  ## e.g:
  ## initContainers:
  ##  - name: your-image-name
  ##    image: your-image
  ##    imagePullPolicy: Always
  ##    command: ['sh', '-c', 'echo "hello world"']
  ##
  initContainers: []
  ## Service account for Index Coordinator to use
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param indexCoord.serviceAccount.create Enable creation of ServiceAccount for Index Coordinator pods
    ##
    create: true
    ## @param indexCoord.serviceAccount.name The name of the ServiceAccount to use
    ## If not set and create is true, a name is generated using the common.names.fullname template
    ##
    name: ""
    ## @param indexCoord.serviceAccount.automountServiceAccountToken Allows auto mount of ServiceAccountToken on the serviceAccount created
    ## Can be set to false if pods using this serviceAccount do not need to use K8s API
    ##
    automountServiceAccountToken: false
    ## @param indexCoord.serviceAccount.annotations Additional custom annotations for the ServiceAccount
    ##
    annotations: {}
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
  ## @param indexCoord.pdb.create Enable/disable a Pod Disruption Budget creation
  ## @param indexCoord.pdb.minAvailable [object] Minimum number/percentage of pods that should remain scheduled
  ## @param indexCoord.pdb.maxUnavailable [object] Maximum number/percentage of pods that may be made unavailable.Defaults to `1` if both `queryCoord.pdb.minAvailable` and `queryCoord.pdb.maxUnavailable` are empty.
  ##
  pdb:
    create: true
    minAvailable: ""
    maxUnavailable: ""
  ## @section Index Coordinator Autoscaling configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ##
  autoscaling:
    vpa:
      ## @param indexCoord.autoscaling.vpa.enabled Enable VPA
      ##
      enabled: false
      ## @param indexCoord.autoscaling.vpa.annotations Annotations for VPA resource
      ##
      annotations: {}
      ## @param indexCoord.autoscaling.vpa.controlledResources VPA List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
      ##
      controlledResources: []
      ## @param indexCoord.autoscaling.vpa.maxAllowed VPA Max allowed resources for the pod
      ## cpu: 200m
      ## memory: 100Mi
      maxAllowed: {}
      ## @param indexCoord.autoscaling.vpa.minAllowed VPA Min allowed resources for the pod
      ## cpu: 200m
      ## memory: 100Mi
      minAllowed: {}
      updatePolicy:
        ## @param indexCoord.autoscaling.vpa.updatePolicy.updateMode Autoscaling update policy Specifies whether recommended updates are applied when a Pod is started and whether recommended updates are applied during the life of a Pod
        ## Possible values are "Off", "Initial", "Recreate", and "Auto".
        ##
        updateMode: Auto
    hpa:
      ## @param indexCoord.autoscaling.hpa.enabled Enable HPA for Milvus Data Plane
      ##
      enabled: false
      ## @param indexCoord.autoscaling.hpa.annotations Annotations for HPA resource
      ##
      annotations: {}
      ## @param indexCoord.autoscaling.hpa.minReplicas Minimum number of Milvus Data Plane replicas
      ##
      minReplicas: ""
      ## @param indexCoord.autoscaling.hpa.maxReplicas Maximum number of Milvus Data Plane replicas
      ##
      maxReplicas: ""
      ## @param indexCoord.autoscaling.hpa.targetCPU Target CPU utilization percentage
      ##
      targetCPU: ""
      ## @param indexCoord.autoscaling.hpa.targetMemory Target Memory utilization percentage
      ##
      targetMemory: ""
  ## @section Index Coordinator Traffic Exposure Parameters
  ##

  ## data coordinator service parameters
  ##
  service:
    ## @param indexCoord.service.type Index Coordinator service type
    ##
    type: ClusterIP
    ## @param indexCoord.service.ports.grpc Index Coordinator GRPC service port
    ## @param indexCoord.service.ports.metrics Index Coordinator Metrics service port
    ##
    ports:
      grpc: 19530
      metrics: 9091
    ## Node ports to expose
    ## NOTE: choose port between <30000-32767>
    ## @param indexCoord.service.nodePorts.grpc Node port for GRPC
    ## @param indexCoord.service.nodePorts.metrics Node port for Metrics
    ##
    nodePorts:
      grpc: ""
      metrics: ""
    ## @param indexCoord.service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}
    ## @param indexCoord.service.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param indexCoord.service.clusterIP Index Coordinator service Cluster IP
    ## e.g.:
    ## clusterIP: None
    ##
    clusterIP: ""
    ## @param indexCoord.service.loadBalancerIP Index Coordinator service Load Balancer IP
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
    ##
    loadBalancerIP: ""
    ## @param indexCoord.service.loadBalancerSourceRanges Index Coordinator service Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## e.g:
    ## loadBalancerSourceRanges:
    ##   - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## @param indexCoord.service.externalTrafficPolicy Index Coordinator service external traffic policy
    ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param indexCoord.service.annotations Additional custom annotations for Index Coordinator service
    ##
    annotations: {}
    ## @param indexCoord.service.extraPorts Extra ports to expose in the Index Coordinator service
    ##
    extraPorts: []
  ## Network Policy configuration
  ## ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param indexCoord.networkPolicy.enabled Enable creation of NetworkPolicy resources
    ##
    enabled: true
    ## @param indexCoord.networkPolicy.allowExternal The Policy model to apply
    ## When set to false, only pods with the correct client label will have network access to the ports Milvus is
    ## listening on. When true, Milvus will accept connections from any source (with the correct destination port).
    ##
    allowExternal: true
    ## @param indexCoord.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param indexCoord.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraIngress: []
    ## @param indexCoord.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param indexCoord.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces
    ## @param indexCoord.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces
    ##
    ingressNSMatchLabels: {}
    ingressNSPodMatchLabels: {}
  ## @section Index Coordinator Metrics Parameters
  ## Prometheus Exporter / Metrics
  ##
  metrics:
    ## @param indexCoord.metrics.enabled Enable metrics
    ##
    enabled: false
    ## @param indexCoord.metrics.annotations [object] Annotations for the server service in order to scrape metrics
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "{{ .Values.indexCoord.service.ports.grpc }}"
    ## Prometheus Operator ServiceMonitor configuration
    ##
    serviceMonitor:
      ## @param indexCoord.metrics.serviceMonitor.enabled Create ServiceMonitor Resource for scraping metrics using Prometheus Operator
      ##
      enabled: false
      ## @param indexCoord.metrics.serviceMonitor.annotations Annotations for the ServiceMonitor Resource
      ##
      annotations: ""
      ## @param indexCoord.metrics.serviceMonitor.namespace Namespace for the ServiceMonitor Resource (defaults to the Release Namespace)
      ##
      namespace: ""
      ## @param indexCoord.metrics.serviceMonitor.interval Interval at which metrics should be scraped.
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      interval: ""
      ## @param indexCoord.metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      scrapeTimeout: ""
      ## @param indexCoord.metrics.serviceMonitor.labels Additional labels that can be used so ServiceMonitor will be discovered by Prometheus
      ##
      labels: {}
      ## @param indexCoord.metrics.serviceMonitor.selector Prometheus instance selector labels
      ## ref: https://github.com/bitnami/charts/tree/main/bitnami/prometheus-operator#prometheus-configuration
      ##
      selector: {}
      ## @param indexCoord.metrics.serviceMonitor.relabelings RelabelConfigs to apply to samples before scraping
      ##
      relabelings: []
      ## @param indexCoord.metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion
      ##
      metricRelabelings: []
      ## @param indexCoord.metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint
      ##
      honorLabels: false
      ## @param indexCoord.metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in prometheus.
      ##
      jobLabel: ""
## @section Data Node Deployment Parameters
##
dataNode:
  ## @param dataNode.enabled Enable Data Node deployment
  ##
  enabled: true
  ## @param dataNode.extraEnvVars Array with extra environment variables to add to data node nodes
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  ## @param dataNode.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for data node nodes
  ##
  extraEnvVarsCM: ""
  ## @param dataNode.extraEnvVarsSecret Name of existing Secret containing extra env vars for data node nodes
  ##
  extraEnvVarsSecret: ""
  ## @param dataNode.defaultConfig [string] Default override configuration from the common set in milvus.defaultConfig
  ##
  defaultConfig: |
    # Override the port for internal binding (the external components will use the service port defined in milvus.defaultConfig)
    dataNode:
      port: {{ .Values.dataNode.containerPorts.grpc }}
      enableDisk: true
  ## @param dataNode.existingConfigMap name of a ConfigMap with existing configuration for the default configuration
  ##
  existingConfigMap: ""
  ## @param dataNode.extraConfig Override configuration
  ##
  extraConfig: {}
  ## @param dataNode.extraConfigExistingConfigMap name of a ConfigMap with existing configuration for the Dashboard
  ##
  extraConfigExistingConfigMap: ""
  ## @param dataNode.command Override default container command (useful when using custom images)
  ##
  command: []
  ## @param dataNode.args Override default container args (useful when using custom images)
  ##
  args: []
  ## @param dataNode.replicaCount Number of Data Node replicas to deploy
  ##
  replicaCount: 1
  ## @param dataNode.containerPorts.grpc GRPC port for Data Node
  ## @param dataNode.containerPorts.metrics Metrics port for Data Node
  containerPorts:
    grpc: 19530
    metrics: 9091
  ## Configure extra options for Data Node containers' liveness, readiness and startup probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
  ## @param dataNode.livenessProbe.enabled Enable livenessProbe on Data Node nodes
  ## @param dataNode.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param dataNode.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param dataNode.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param dataNode.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param dataNode.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param dataNode.readinessProbe.enabled Enable readinessProbe on Data Node nodes
  ## @param dataNode.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param dataNode.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param dataNode.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param dataNode.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param dataNode.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param dataNode.startupProbe.enabled Enable startupProbe on Data Node containers
  ## @param dataNode.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param dataNode.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param dataNode.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param dataNode.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param dataNode.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param dataNode.customLivenessProbe Custom livenessProbe that overrides the default one
  ##
  customLivenessProbe: {}
  ## @param dataNode.customReadinessProbe Custom readinessProbe that overrides the default one
  ##
  customReadinessProbe: {}
  ## @param dataNode.customStartupProbe Custom startupProbe that overrides the default one
  ##
  customStartupProbe: {}
  ## data node resource requests and limits
  ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param dataNode.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if dataNode.resources is set (dataNode.resources is recommended for production).
  ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
  ##
  resourcesPreset: "micro"
  ## @param dataNode.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param dataNode.podSecurityContext.enabled Enabled Data Node pods' Security Context
  ## @param dataNode.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
  ## @param dataNode.podSecurityContext.sysctls Set kernel settings using the sysctl interface
  ## @param dataNode.podSecurityContext.supplementalGroups Set filesystem extra groups
  ## @param dataNode.podSecurityContext.fsGroup Set Data Node pod's Security Context fsGroup
  ##
  podSecurityContext:
    enabled: true
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## Configure Container Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param dataNode.containerSecurityContext.enabled Enabled containers' Security Context
  ## @param dataNode.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
  ## @param dataNode.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
  ## @param dataNode.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
  ## @param dataNode.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
  ## @param dataNode.containerSecurityContext.privileged Set container's Security Context privileged
  ## @param dataNode.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
  ## @param dataNode.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
  ## @param dataNode.containerSecurityContext.capabilities.drop List of capabilities to be dropped
  ## @param dataNode.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
  ##
  containerSecurityContext:
    enabled: true
    seLinuxOptions: {}
    runAsUser: 1001
    runAsGroup: 1001
    runAsNonRoot: true
    privileged: false
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    capabilities:
      drop: ["ALL"]
    seccompProfile:
      type: "RuntimeDefault"
  ## @param dataNode.lifecycleHooks for the data node container(s) to automate configuration before or after startup
  ##
  lifecycleHooks: {}
  ## @param dataNode.runtimeClassName Name of the runtime class to be used by pod(s)
  ## ref: https://kubernetes.io/docs/concepts/containers/runtime-class/
  ##
  runtimeClassName: ""
  ## @param dataNode.automountServiceAccountToken Mount Service Account token in pod
  ##
  automountServiceAccountToken: false
  ## @param dataNode.hostAliases data node pods host aliases
  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
  ##
  hostAliases: []
  ## @param dataNode.podLabels Extra labels for data node pods
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  ##
  podLabels: {}
  ## @param dataNode.podAnnotations Annotations for data node pods
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  podAnnotations: {}
  ## @param dataNode.podAffinityPreset Pod affinity preset. Ignored if `data node.affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAffinityPreset: ""
  ## @param dataNode.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `data node.affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAntiAffinityPreset: soft
  ## Node data node.affinity preset
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  ##
  nodeAffinityPreset:
    ## @param dataNode.nodeAffinityPreset.type Node affinity preset type. Ignored if `data node.affinity` is set. Allowed values: `soft` or `hard`
    ##
    type: ""
    ## @param dataNode.nodeAffinityPreset.key Node label key to match. Ignored if `data node.affinity` is set
    ##
    key: ""
    ## @param dataNode.nodeAffinityPreset.values Node label values to match. Ignored if `data node.affinity` is set
    ## E.g.
    ## values:
    ##   - e2e-az1
    ##   - e2e-az2
    ##
    values: []
  ## @param dataNode.affinity Affinity for Data Node pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## NOTE: `dataNode.podAffinityPreset`, `dataNode.podAntiAffinityPreset`, and `dataNode.nodeAffinityPreset` will be ignored when it's set
  ##
  affinity: {}
  ## @param dataNode.nodeSelector Node labels for Data Node pods assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  ##
  nodeSelector: {}
  ## @param dataNode.tolerations Tolerations for Data Node pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param dataNode.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
  ##
  topologySpreadConstraints: []
  ## @param dataNode.priorityClassName Data Node pods' priorityClassName
  ##
  priorityClassName: ""
  ## @param dataNode.schedulerName Kubernetes pod scheduler registry
  ## https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""
  ## @param dataNode.updateStrategy.type Data Node statefulset strategy type
  ## @param dataNode.updateStrategy.rollingUpdate Data Node statefulset rolling update configuration parameters
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ##
  updateStrategy:
    type: RollingUpdate
    rollingUpdate: {}
  ## @param dataNode.extraVolumes Optionally specify extra list of additional volumes for the Data Node pod(s)
  ##
  extraVolumes: []
  ## @param dataNode.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the Data Node container(s)
  ##
  extraVolumeMounts: []
  ## @param dataNode.sidecars Add additional sidecar containers to the Data Node pod(s)
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## @param dataNode.enableDefaultInitContainers Deploy default init containers
  ##
  enableDefaultInitContainers: true
  ## @param dataNode.initContainers Add additional init containers to the Data Node pod(s)
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
  ## e.g:
  ## initContainers:
  ##  - name: your-image-name
  ##    image: your-image
  ##    imagePullPolicy: Always
  ##    command: ['sh', '-c', 'echo "hello world"']
  ##
  initContainers: []
  ## Service account for Data Node to use
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param dataNode.serviceAccount.create Enable creation of ServiceAccount for Data Node pods
    ##
    create: true
    ## @param dataNode.serviceAccount.name The name of the ServiceAccount to use
    ## If not set and create is true, a name is generated using the common.names.fullname template
    ##
    name: ""
    ## @param dataNode.serviceAccount.automountServiceAccountToken Allows auto mount of ServiceAccountToken on the serviceAccount created
    ## Can be set to false if pods using this serviceAccount do not need to use K8s API
    ##
    automountServiceAccountToken: false
    ## @param dataNode.serviceAccount.annotations Additional custom annotations for the ServiceAccount
    ##
    annotations: {}
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
  ## @param dataNode.pdb.create Enable/disable a Pod Disruption Budget creation
  ## @param dataNode.pdb.minAvailable [object] Minimum number/percentage of pods that should remain scheduled
  ## @param dataNode.pdb.maxUnavailable [object] Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `dataNode.pdb.minAvailable` and `dataNode.pdb.maxUnavailable` are empty.
  ##
  pdb:
    create: true
    minAvailable: ""
    maxUnavailable: ""
  ## @section Data Node Autoscaling configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ##
  autoscaling:
    vpa:
      ## @param dataNode.autoscaling.vpa.enabled Enable VPA
      ##
      enabled: false
      ## @param dataNode.autoscaling.vpa.annotations Annotations for VPA resource
      ##
      annotations: {}
      ## @param dataNode.autoscaling.vpa.controlledResources VPA List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
      ##
      controlledResources: []
      ## @param dataNode.autoscaling.vpa.maxAllowed VPA Max allowed resources for the pod
      ## cpu: 200m
      ## memory: 100Mi
      maxAllowed: {}
      ## @param dataNode.autoscaling.vpa.minAllowed VPA Min allowed resources for the pod
      ## cpu: 200m
      ## memory: 100Mi
      minAllowed: {}
      updatePolicy:
        ## @param dataNode.autoscaling.vpa.updatePolicy.updateMode Autoscaling update policy Specifies whether recommended updates are applied when a Pod is started and whether recommended updates are applied during the life of a Pod
        ## Possible values are "Off", "Initial", "Recreate", and "Auto".
        ##
        updateMode: Auto
    hpa:
      ## @param dataNode.autoscaling.hpa.enabled Enable HPA for Milvus Data Plane
      ##
      enabled: false
      ## @param dataNode.autoscaling.hpa.annotations Annotations for HPA resource
      ##
      annotations: {}
      ## @param dataNode.autoscaling.hpa.minReplicas Minimum number of Milvus Data Plane replicas
      ##
      minReplicas: ""
      ## @param dataNode.autoscaling.hpa.maxReplicas Maximum number of Milvus Data Plane replicas
      ##
      maxReplicas: ""
      ## @param dataNode.autoscaling.hpa.targetCPU Target CPU utilization percentage
      ##
      targetCPU: ""
      ## @param dataNode.autoscaling.hpa.targetMemory Target Memory utilization percentage
      ##
      targetMemory: ""
  ## @section Data Node Traffic Exposure Parameters
  ##

  ## data node service parameters
  ##
  service:
    ## @param dataNode.service.type Data Node service type
    ##
    type: ClusterIP
    ## @param dataNode.service.ports.grpc Data Node GRPC service port
    ## @param dataNode.service.ports.metrics Data Node Metrics service port
    ##
    ports:
      grpc: 19530
      metrics: 9091
    ## Node ports to expose
    ## NOTE: choose port between <30000-32767>
    ## @param dataNode.service.nodePorts.grpc Node port for GRPC
    ## @param dataNode.service.nodePorts.metrics Node port for Metrics
    ##
    nodePorts:
      grpc: ""
      metrics: ""
    ## @param dataNode.service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}
    ## @param dataNode.service.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param dataNode.service.clusterIP Data Node service Cluster IP
    ## e.g.:
    ## clusterIP: None
    ##
    clusterIP: ""
    ## @param dataNode.service.loadBalancerIP Data Node service Load Balancer IP
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
    ##
    loadBalancerIP: ""
    ## @param dataNode.service.loadBalancerSourceRanges Data Node service Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## e.g:
    ## loadBalancerSourceRanges:
    ##   - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## @param dataNode.service.externalTrafficPolicy Data Node service external traffic policy
    ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param dataNode.service.annotations Additional custom annotations for Data Node service
    ##
    annotations: {}
    ## @param dataNode.service.extraPorts Extra ports to expose in the Data Node service
    ##
    extraPorts: []
  ## Network Policy configuration
  ## ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param dataNode.networkPolicy.enabled Enable creation of NetworkPolicy resources
    ##
    enabled: true
    ## @param dataNode.networkPolicy.allowExternal The Policy model to apply
    ## When set to false, only pods with the correct client label will have network access to the ports Milvus is
    ## listening on. When true, Milvus will accept connections from any source (with the correct destination port).
    ##
    allowExternal: true
    ## @param dataNode.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param dataNode.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraIngress: []
    ## @param dataNode.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param dataNode.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces
    ## @param dataNode.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces
    ##
    ingressNSMatchLabels: {}
    ingressNSPodMatchLabels: {}
  ## @section Data Node Metrics Parameters
  ## Prometheus Exporter / Metrics
  ##
  metrics:
    ## @param dataNode.metrics.enabled Enable metrics
    ##
    enabled: false
    ## @param dataNode.metrics.annotations [object] Annotations for the server service in order to scrape metrics
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "{{ .Values.dataNode.service.ports.grpc }}"
    ## Prometheus Operator ServiceMonitor configuration
    ##
    serviceMonitor:
      ## @param dataNode.metrics.serviceMonitor.enabled Create ServiceMonitor Resource for scraping metrics using Prometheus Operator
      ##
      enabled: false
      ## @param dataNode.metrics.serviceMonitor.annotations Annotations for the ServiceMonitor Resource
      ##
      annotations: ""
      ## @param dataNode.metrics.serviceMonitor.namespace Namespace for the ServiceMonitor Resource (defaults to the Release Namespace)
      ##
      namespace: ""
      ## @param dataNode.metrics.serviceMonitor.interval Interval at which metrics should be scraped.
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      interval: ""
      ## @param dataNode.metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      scrapeTimeout: ""
      ## @param dataNode.metrics.serviceMonitor.labels Additional labels that can be used so ServiceMonitor will be discovered by Prometheus
      ##
      labels: {}
      ## @param dataNode.metrics.serviceMonitor.selector Prometheus instance selector labels
      ## ref: https://github.com/bitnami/charts/tree/main/bitnami/prometheus-operator#prometheus-configuration
      ##
      selector: {}
      ## @param dataNode.metrics.serviceMonitor.relabelings RelabelConfigs to apply to samples before scraping
      ##
      relabelings: []
      ## @param dataNode.metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion
      ##
      metricRelabelings: []
      ## @param dataNode.metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint
      ##
      honorLabels: false
      ## @param dataNode.metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in prometheus.
      ##
      jobLabel: ""
## @section Query Node Deployment Parameters
##
queryNode:
  ## @param queryNode.enabled Enable Query Node deployment
  ##
  enabled: true
  ## @param queryNode.extraEnvVars Array with extra environment variables to add to data node nodes
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  ## @param queryNode.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for data node nodes
  ##
  extraEnvVarsCM: ""
  ## @param queryNode.extraEnvVarsSecret Name of existing Secret containing extra env vars for data node nodes
  ##
  extraEnvVarsSecret: ""
  ## @param queryNode.defaultConfig [string] Default override configuration from the common set in milvus.defaultConfig
  ##
  defaultConfig: |
    # Override the port for internal binding (the external components will use the service port defined in milvus.defaultConfig)
    queryNode:
      port: {{ .Values.queryNode.containerPorts.grpc }}
      enableDisk: true
  ## @param queryNode.existingConfigMap name of a ConfigMap with existing configuration for the default configuration
  ##
  existingConfigMap: ""
  ## @param queryNode.extraConfig Override configuration
  ##
  extraConfig: {}
  ## @param queryNode.extraConfigExistingConfigMap name of a ConfigMap with existing configuration for the Dashboard
  ##
  extraConfigExistingConfigMap: ""
  ## @param queryNode.command Override default container command (useful when using custom images)
  ##
  command: []
  ## @param queryNode.args Override default container args (useful when using custom images)
  ##
  args: []
  ## @param queryNode.replicaCount Number of Query Node replicas to deploy
  ##
  replicaCount: 1
  ## @param queryNode.containerPorts.grpc GRPC port for Query Node
  ## @param queryNode.containerPorts.metrics Metrics port for Query Node
  containerPorts:
    grpc: 19530
    metrics: 9091
  ## Configure extra options for Query Node containers' liveness, readiness and startup probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
  ## @param queryNode.livenessProbe.enabled Enable livenessProbe on Query Node nodes
  ## @param queryNode.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param queryNode.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param queryNode.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param queryNode.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param queryNode.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param queryNode.readinessProbe.enabled Enable readinessProbe on Query Node nodes
  ## @param queryNode.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param queryNode.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param queryNode.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param queryNode.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param queryNode.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param queryNode.startupProbe.enabled Enable startupProbe on Query Node containers
  ## @param queryNode.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param queryNode.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param queryNode.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param queryNode.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param queryNode.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param queryNode.customLivenessProbe Custom livenessProbe that overrides the default one
  ##
  customLivenessProbe: {}
  ## @param queryNode.customReadinessProbe Custom readinessProbe that overrides the default one
  ##
  customReadinessProbe: {}
  ## @param queryNode.customStartupProbe Custom startupProbe that overrides the default one
  ##
  customStartupProbe: {}
  ## data node resource requests and limits
  ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param queryNode.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if queryNode.resources is set (queryNode.resources is recommended for production).
  ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
  ##
  resourcesPreset: "micro"
  ## @param queryNode.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param queryNode.podSecurityContext.enabled Enabled Query Node pods' Security Context
  ## @param queryNode.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
  ## @param queryNode.podSecurityContext.sysctls Set kernel settings using the sysctl interface
  ## @param queryNode.podSecurityContext.supplementalGroups Set filesystem extra groups
  ## @param queryNode.podSecurityContext.fsGroup Set Query Node pod's Security Context fsGroup
  ##
  podSecurityContext:
    enabled: true
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## Configure Container Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param queryNode.containerSecurityContext.enabled Enabled containers' Security Context
  ## @param queryNode.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
  ## @param queryNode.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
  ## @param queryNode.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
  ## @param queryNode.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
  ## @param queryNode.containerSecurityContext.privileged Set container's Security Context privileged
  ## @param queryNode.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
  ## @param queryNode.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
  ## @param queryNode.containerSecurityContext.capabilities.drop List of capabilities to be dropped
  ## @param queryNode.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
  ##
  containerSecurityContext:
    enabled: true
    seLinuxOptions: {}
    runAsUser: 1001
    runAsGroup: 1001
    runAsNonRoot: true
    privileged: false
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    capabilities:
      drop: ["ALL"]
    seccompProfile:
      type: "RuntimeDefault"
  ## @param queryNode.lifecycleHooks for the data node container(s) to automate configuration before or after startup
  ##
  lifecycleHooks: {}
  ## @param queryNode.runtimeClassName Name of the runtime class to be used by pod(s)
  ## ref: https://kubernetes.io/docs/concepts/containers/runtime-class/
  ##
  runtimeClassName: ""
  ## @param queryNode.automountServiceAccountToken Mount Service Account token in pod
  ##
  automountServiceAccountToken: false
  ## @param queryNode.hostAliases data node pods host aliases
  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
  ##
  hostAliases: []
  ## @param queryNode.podLabels Extra labels for data node pods
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  ##
  podLabels: {}
  ## @param queryNode.podAnnotations Annotations for data node pods
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  podAnnotations: {}
  ## @param queryNode.podAffinityPreset Pod affinity preset. Ignored if `data node.affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAffinityPreset: ""
  ## @param queryNode.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `data node.affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAntiAffinityPreset: soft
  ## Node data node.affinity preset
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  ##
  nodeAffinityPreset:
    ## @param queryNode.nodeAffinityPreset.type Node affinity preset type. Ignored if `data node.affinity` is set. Allowed values: `soft` or `hard`
    ##
    type: ""
    ## @param queryNode.nodeAffinityPreset.key Node label key to match. Ignored if `data node.affinity` is set
    ##
    key: ""
    ## @param queryNode.nodeAffinityPreset.values Node label values to match. Ignored if `data node.affinity` is set
    ## E.g.
    ## values:
    ##   - e2e-az1
    ##   - e2e-az2
    ##
    values: []
  ## @param queryNode.affinity Affinity for Query Node pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## NOTE: `queryNode.podAffinityPreset`, `queryNode.podAntiAffinityPreset`, and `queryNode.nodeAffinityPreset` will be ignored when it's set
  ##
  affinity: {}
  ## @param queryNode.nodeSelector Node labels for Query Node pods assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  ##
  nodeSelector: {}
  ## @param queryNode.tolerations Tolerations for Query Node pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param queryNode.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
  ##
  topologySpreadConstraints: []
  ## @param queryNode.priorityClassName Query Node pods' priorityClassName
  ##
  priorityClassName: ""
  ## @param queryNode.schedulerName Kubernetes pod scheduler registry
  ## https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""
  ## @param queryNode.updateStrategy.type Query Node statefulset strategy type
  ## @param queryNode.updateStrategy.rollingUpdate Query Node statefulset rolling update configuration parameters
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ##
  updateStrategy:
    type: RollingUpdate
    rollingUpdate: {}
  ## @param queryNode.extraVolumes Optionally specify extra list of additional volumes for the Query Node pod(s)
  ##
  extraVolumes: []
  ## @param queryNode.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the Query Node container(s)
  ##
  extraVolumeMounts: []
  ## @param queryNode.sidecars Add additional sidecar containers to the Query Node pod(s)
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## @param queryNode.enableDefaultInitContainers Deploy default init containers
  ##
  enableDefaultInitContainers: true
  ## @param queryNode.initContainers Add additional init containers to the Query Node pod(s)
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
  ## e.g:
  ## initContainers:
  ##  - name: your-image-name
  ##    image: your-image
  ##    imagePullPolicy: Always
  ##    command: ['sh', '-c', 'echo "hello world"']
  ##
  initContainers: []
  ## Service account for Query Node to use
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param queryNode.serviceAccount.create Enable creation of ServiceAccount for Query Node pods
    ##
    create: true
    ## @param queryNode.serviceAccount.name The name of the ServiceAccount to use
    ## If not set and create is true, a name is generated using the common.names.fullname template
    ##
    name: ""
    ## @param queryNode.serviceAccount.automountServiceAccountToken Allows auto mount of ServiceAccountToken on the serviceAccount created
    ## Can be set to false if pods using this serviceAccount do not need to use K8s API
    ##
    automountServiceAccountToken: false
    ## @param queryNode.serviceAccount.annotations Additional custom annotations for the ServiceAccount
    ##
    annotations: {}
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
  ## @param queryNode.pdb.create Enable/disable a Pod Disruption Budget creation
  ## @param queryNode.pdb.minAvailable [object] Minimum number/percentage of pods that should remain scheduled
  ## @param queryNode.pdb.maxUnavailable [object] Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `queryNode.pdb.minAvailable` and `queryNode.pdb.maxUnavailable` are empty.
  ##
  pdb:
    create: true
    minAvailable: ""
    maxUnavailable: ""
  ## @section Query Node Autoscaling configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ##
  autoscaling:
    vpa:
      ## @param queryNode.autoscaling.vpa.enabled Enable VPA
      ##
      enabled: false
      ## @param queryNode.autoscaling.vpa.annotations Annotations for VPA resource
      ##
      annotations: {}
      ## @param queryNode.autoscaling.vpa.controlledResources VPA List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
      ##
      controlledResources: []
      ## @param queryNode.autoscaling.vpa.maxAllowed VPA Max allowed resources for the pod
      ## cpu: 200m
      ## memory: 100Mi
      maxAllowed: {}
      ## @param queryNode.autoscaling.vpa.minAllowed VPA Min allowed resources for the pod
      ## cpu: 200m
      ## memory: 100Mi
      minAllowed: {}
      updatePolicy:
        ## @param queryNode.autoscaling.vpa.updatePolicy.updateMode Autoscaling update policy Specifies whether recommended updates are applied when a Pod is started and whether recommended updates are applied during the life of a Pod
        ## Possible values are "Off", "Initial", "Recreate", and "Auto".
        ##
        updateMode: Auto
    hpa:
      ## @param queryNode.autoscaling.hpa.enabled Enable HPA for Milvus Data Plane
      ##
      enabled: false
      ## @param queryNode.autoscaling.hpa.annotations Annotations for HPA resource
      ##
      annotations: {}
      ## @param queryNode.autoscaling.hpa.minReplicas Minimum number of Milvus Data Plane replicas
      ##
      minReplicas: ""
      ## @param queryNode.autoscaling.hpa.maxReplicas Maximum number of Milvus Data Plane replicas
      ##
      maxReplicas: ""
      ## @param queryNode.autoscaling.hpa.targetCPU Target CPU utilization percentage
      ##
      targetCPU: ""
      ## @param queryNode.autoscaling.hpa.targetMemory Target Memory utilization percentage
      ##
      targetMemory: ""
  ## @section Query Node Traffic Exposure Parameters
  ##

  ## data node service parameters
  ##
  service:
    ## @param queryNode.service.type Query Node service type
    ##
    type: ClusterIP
    ## @param queryNode.service.ports.grpc Query Node GRPC service port
    ## @param queryNode.service.ports.metrics Query Node Metrics service port
    ##
    ports:
      grpc: 19530
      metrics: 9091
    ## Node ports to expose
    ## NOTE: choose port between <30000-32767>
    ## @param queryNode.service.nodePorts.grpc Node port for GRPC
    ## @param queryNode.service.nodePorts.metrics Node port for Metrics
    ##
    nodePorts:
      grpc: ""
      metrics: ""
    ## @param queryNode.service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}
    ## @param queryNode.service.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param queryNode.service.clusterIP Query Node service Cluster IP
    ## e.g.:
    ## clusterIP: None
    ##
    clusterIP: ""
    ## @param queryNode.service.loadBalancerIP Query Node service Load Balancer IP
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
    ##
    loadBalancerIP: ""
    ## @param queryNode.service.loadBalancerSourceRanges Query Node service Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## e.g:
    ## loadBalancerSourceRanges:
    ##   - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## @param queryNode.service.externalTrafficPolicy Query Node service external traffic policy
    ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param queryNode.service.annotations Additional custom annotations for Query Node service
    ##
    annotations: {}
    ## @param queryNode.service.extraPorts Extra ports to expose in the Query Node service
    ##
    extraPorts: []
  ## Network Policy configuration
  ## ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param queryNode.networkPolicy.enabled Enable creation of NetworkPolicy resources
    ##
    enabled: true
    ## @param queryNode.networkPolicy.allowExternal The Policy model to apply
    ## When set to false, only pods with the correct client label will have network access to the ports Milvus is
    ## listening on. When true, Milvus will accept connections from any source (with the correct destination port).
    ##
    allowExternal: true
    ## @param queryNode.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param queryNode.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraIngress: []
    ## @param queryNode.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param queryNode.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces
    ## @param queryNode.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces
    ##
    ingressNSMatchLabels: {}
    ingressNSPodMatchLabels: {}
  ## @section Query Node Metrics Parameters
  ## Prometheus Exporter / Metrics
  ##
  metrics:
    ## @param queryNode.metrics.enabled Enable metrics
    ##
    enabled: false
    ## @param queryNode.metrics.annotations [object] Annotations for the server service in order to scrape metrics
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "{{ .Values.queryNode.service.ports.grpc }}"
    ## Prometheus Operator ServiceMonitor configuration
    ##
    serviceMonitor:
      ## @param queryNode.metrics.serviceMonitor.enabled Create ServiceMonitor Resource for scraping metrics using Prometheus Operator
      ##
      enabled: false
      ## @param queryNode.metrics.serviceMonitor.annotations Annotations for the ServiceMonitor Resource
      ##
      annotations: ""
      ## @param queryNode.metrics.serviceMonitor.namespace Namespace for the ServiceMonitor Resource (defaults to the Release Namespace)
      ##
      namespace: ""
      ## @param queryNode.metrics.serviceMonitor.interval Interval at which metrics should be scraped.
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      interval: ""
      ## @param queryNode.metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      scrapeTimeout: ""
      ## @param queryNode.metrics.serviceMonitor.labels Additional labels that can be used so ServiceMonitor will be discovered by Prometheus
      ##
      labels: {}
      ## @param queryNode.metrics.serviceMonitor.selector Prometheus instance selector labels
      ## ref: https://github.com/bitnami/charts/tree/main/bitnami/prometheus-operator#prometheus-configuration
      ##
      selector: {}
      ## @param queryNode.metrics.serviceMonitor.relabelings RelabelConfigs to apply to samples before scraping
      ##
      relabelings: []
      ## @param queryNode.metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion
      ##
      metricRelabelings: []
      ## @param queryNode.metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint
      ##
      honorLabels: false
      ## @param queryNode.metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in prometheus.
      ##
      jobLabel: ""
## @section Index Node Deployment Parameters
##
indexNode:
  ## @param indexNode.enabled Enable Index Node deployment
  ##
  enabled: true
  ## @param indexNode.extraEnvVars Array with extra environment variables to add to data node nodes
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  ## @param indexNode.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for data node nodes
  ##
  extraEnvVarsCM: ""
  ## @param indexNode.extraEnvVarsSecret Name of existing Secret containing extra env vars for data node nodes
  ##
  extraEnvVarsSecret: ""
  ## @param indexNode.defaultConfig [string] Default override configuration from the common set in milvus.defaultConfig
  ##
  defaultConfig: |
    # Override the port for internal binding (the external components will use the service port defined in milvus.defaultConfig)
    indexNode:
      port: {{ .Values.indexNode.containerPorts.grpc }}
      enableDisk: true
  ## @param indexNode.existingConfigMap name of a ConfigMap with existing configuration for the default configuration
  ##
  existingConfigMap: ""
  ## @param indexNode.extraConfig Override configuration
  ##
  extraConfig: {}
  ## @param indexNode.extraConfigExistingConfigMap name of a ConfigMap with existing configuration for the Dashboard
  ##
  extraConfigExistingConfigMap: ""
  ## @param indexNode.command Override default container command (useful when using custom images)
  ##
  command: []
  ## @param indexNode.args Override default container args (useful when using custom images)
  ##
  args: []
  ## @param indexNode.replicaCount Number of Index Node replicas to deploy
  ##
  replicaCount: 1
  ## @param indexNode.containerPorts.grpc GRPC port for Index Node
  ## @param indexNode.containerPorts.metrics Metrics port for Index Node
  containerPorts:
    grpc: 19530
    metrics: 9091
  ## Configure extra options for Index Node containers' liveness, readiness and startup probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
  ## @param indexNode.livenessProbe.enabled Enable livenessProbe on Index Node nodes
  ## @param indexNode.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param indexNode.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param indexNode.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param indexNode.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param indexNode.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param indexNode.readinessProbe.enabled Enable readinessProbe on Index Node nodes
  ## @param indexNode.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param indexNode.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param indexNode.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param indexNode.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param indexNode.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param indexNode.startupProbe.enabled Enable startupProbe on Index Node containers
  ## @param indexNode.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param indexNode.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param indexNode.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param indexNode.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param indexNode.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param indexNode.customLivenessProbe Custom livenessProbe that overrides the default one
  ##
  customLivenessProbe: {}
  ## @param indexNode.customReadinessProbe Custom readinessProbe that overrides the default one
  ##
  customReadinessProbe: {}
  ## @param indexNode.customStartupProbe Custom startupProbe that overrides the default one
  ##
  customStartupProbe: {}
  ## data node resource requests and limits
  ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param indexNode.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if indexNode.resources is set (indexNode.resources is recommended for production).
  ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
  ##
  resourcesPreset: "micro"
  ## @param indexNode.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param indexNode.podSecurityContext.enabled Enabled Index Node pods' Security Context
  ## @param indexNode.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
  ## @param indexNode.podSecurityContext.sysctls Set kernel settings using the sysctl interface
  ## @param indexNode.podSecurityContext.supplementalGroups Set filesystem extra groups
  ## @param indexNode.podSecurityContext.fsGroup Set Index Node pod's Security Context fsGroup
  ##
  podSecurityContext:
    enabled: true
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## Configure Container Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param indexNode.containerSecurityContext.enabled Enabled containers' Security Context
  ## @param indexNode.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
  ## @param indexNode.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
  ## @param indexNode.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
  ## @param indexNode.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
  ## @param indexNode.containerSecurityContext.privileged Set container's Security Context privileged
  ## @param indexNode.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
  ## @param indexNode.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
  ## @param indexNode.containerSecurityContext.capabilities.drop List of capabilities to be dropped
  ## @param indexNode.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
  ##
  containerSecurityContext:
    enabled: true
    seLinuxOptions: {}
    runAsUser: 1001
    runAsGroup: 1001
    runAsNonRoot: true
    privileged: false
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    capabilities:
      drop: ["ALL"]
    seccompProfile:
      type: "RuntimeDefault"
  ## @param indexNode.lifecycleHooks for the data node container(s) to automate configuration before or after startup
  ##
  lifecycleHooks: {}
  ## @param indexNode.runtimeClassName Name of the runtime class to be used by pod(s)
  ## ref: https://kubernetes.io/docs/concepts/containers/runtime-class/
  ##
  runtimeClassName: ""
  ## @param indexNode.automountServiceAccountToken Mount Service Account token in pod
  ##
  automountServiceAccountToken: false
  ## @param indexNode.hostAliases data node pods host aliases
  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
  ##
  hostAliases: []
  ## @param indexNode.podLabels Extra labels for data node pods
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  ##
  podLabels: {}
  ## @param indexNode.podAnnotations Annotations for data node pods
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  podAnnotations: {}
  ## @param indexNode.podAffinityPreset Pod affinity preset. Ignored if `data node.affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAffinityPreset: ""
  ## @param indexNode.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `data node.affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAntiAffinityPreset: soft
  ## Node data node.affinity preset
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  ##
  nodeAffinityPreset:
    ## @param indexNode.nodeAffinityPreset.type Node affinity preset type. Ignored if `data node.affinity` is set. Allowed values: `soft` or `hard`
    ##
    type: ""
    ## @param indexNode.nodeAffinityPreset.key Node label key to match. Ignored if `data node.affinity` is set
    ##
    key: ""
    ## @param indexNode.nodeAffinityPreset.values Node label values to match. Ignored if `data node.affinity` is set
    ## E.g.
    ## values:
    ##   - e2e-az1
    ##   - e2e-az2
    ##
    values: []
  ## @param indexNode.affinity Affinity for Index Node pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## NOTE: `indexNode.podAffinityPreset`, `indexNode.podAntiAffinityPreset`, and `indexNode.nodeAffinityPreset` will be ignored when it's set
  ##
  affinity: {}
  ## @param indexNode.nodeSelector Node labels for Index Node pods assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  ##
  nodeSelector: {}
  ## @param indexNode.tolerations Tolerations for Index Node pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param indexNode.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
  ##
  topologySpreadConstraints: []
  ## @param indexNode.priorityClassName Index Node pods' priorityClassName
  ##
  priorityClassName: ""
  ## @param indexNode.schedulerName Kubernetes pod scheduler registry
  ## https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""
  ## @param indexNode.updateStrategy.type Index Node statefulset strategy type
  ## @param indexNode.updateStrategy.rollingUpdate Index Node statefulset rolling update configuration parameters
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ##
  updateStrategy:
    type: RollingUpdate
    rollingUpdate: {}
  ## @param indexNode.extraVolumes Optionally specify extra list of additional volumes for the Index Node pod(s)
  ##
  extraVolumes: []
  ## @param indexNode.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the Index Node container(s)
  ##
  extraVolumeMounts: []
  ## @param indexNode.sidecars Add additional sidecar containers to the Index Node pod(s)
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## @param indexNode.enableDefaultInitContainers Deploy default init containers
  ##
  enableDefaultInitContainers: true
  ## @param indexNode.initContainers Add additional init containers to the Index Node pod(s)
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
  ## e.g:
  ## initContainers:
  ##  - name: your-image-name
  ##    image: your-image
  ##    imagePullPolicy: Always
  ##    command: ['sh', '-c', 'echo "hello world"']
  ##
  initContainers: []
  ## Service account for Index Node to use
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param indexNode.serviceAccount.create Enable creation of ServiceAccount for Index Node pods
    ##
    create: true
    ## @param indexNode.serviceAccount.name The name of the ServiceAccount to use
    ## If not set and create is true, a name is generated using the common.names.fullname template
    ##
    name: ""
    ## @param indexNode.serviceAccount.automountServiceAccountToken Allows auto mount of ServiceAccountToken on the serviceAccount created
    ## Can be set to false if pods using this serviceAccount do not need to use K8s API
    ##
    automountServiceAccountToken: false
    ## @param indexNode.serviceAccount.annotations Additional custom annotations for the ServiceAccount
    ##
    annotations: {}
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
  ## @param indexNode.pdb.create Enable/disable a Pod Disruption Budget creation
  ## @param indexNode.pdb.minAvailable [object] Minimum number/percentage of pods that should remain scheduled
  ## @param indexNode.pdb.maxUnavailable [object] Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `indexNode.pdb.minAvailable` and `indexNode.pdb.maxUnavailable` are empty.
  ##
  pdb:
    create: true
    minAvailable: ""
    maxUnavailable: ""
  ## @section Index Node Autoscaling configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ##
  autoscaling:
    vpa:
      ## @param indexNode.autoscaling.vpa.enabled Enable VPA
      ##
      enabled: false
      ## @param indexNode.autoscaling.vpa.annotations Annotations for VPA resource
      ##
      annotations: {}
      ## @param indexNode.autoscaling.vpa.controlledResources VPA List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
      ##
      controlledResources: []
      ## @param indexNode.autoscaling.vpa.maxAllowed VPA Max allowed resources for the pod
      ## cpu: 200m
      ## memory: 100Mi
      maxAllowed: {}
      ## @param indexNode.autoscaling.vpa.minAllowed VPA Min allowed resources for the pod
      ## cpu: 200m
      ## memory: 100Mi
      minAllowed: {}
      updatePolicy:
        ## @param indexNode.autoscaling.vpa.updatePolicy.updateMode Autoscaling update policy Specifies whether recommended updates are applied when a Pod is started and whether recommended updates are applied during the life of a Pod
        ## Possible values are "Off", "Initial", "Recreate", and "Auto".
        ##
        updateMode: Auto
    hpa:
      ## @param indexNode.autoscaling.hpa.enabled Enable HPA for Milvus Data Plane
      ##
      enabled: false
      ## @param indexNode.autoscaling.hpa.annotations Annotations for HPA resource
      ##
      annotations: {}
      ## @param indexNode.autoscaling.hpa.minReplicas Minimum number of Milvus Data Plane replicas
      ##
      minReplicas: ""
      ## @param indexNode.autoscaling.hpa.maxReplicas Maximum number of Milvus Data Plane replicas
      ##
      maxReplicas: ""
      ## @param indexNode.autoscaling.hpa.targetCPU Target CPU utilization percentage
      ##
      targetCPU: ""
      ## @param indexNode.autoscaling.hpa.targetMemory Target Memory utilization percentage
      ##
      targetMemory: ""
  ## @section Index Node Traffic Exposure Parameters
  ##

  ## data node service parameters
  ##
  service:
    ## @param indexNode.service.type Index Node service type
    ##
    type: ClusterIP
    ## @param indexNode.service.ports.grpc Index Node GRPC service port
    ## @param indexNode.service.ports.metrics Index Node Metrics service port
    ##
    ports:
      grpc: 19530
      metrics: 9091
    ## Node ports to expose
    ## NOTE: choose port between <30000-32767>
    ## @param indexNode.service.nodePorts.grpc Node port for GRPC
    ## @param indexNode.service.nodePorts.metrics Node port for Metrics
    ##
    nodePorts:
      grpc: ""
      metrics: ""
    ## @param indexNode.service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}
    ## @param indexNode.service.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param indexNode.service.clusterIP Index Node service Cluster IP
    ## e.g.:
    ## clusterIP: None
    ##
    clusterIP: ""
    ## @param indexNode.service.loadBalancerIP Index Node service Load Balancer IP
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
    ##
    loadBalancerIP: ""
    ## @param indexNode.service.loadBalancerSourceRanges Index Node service Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## e.g:
    ## loadBalancerSourceRanges:
    ##   - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## @param indexNode.service.externalTrafficPolicy Index Node service external traffic policy
    ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param indexNode.service.annotations Additional custom annotations for Index Node service
    ##
    annotations: {}
    ## @param indexNode.service.extraPorts Extra ports to expose in the Index Node service
    ##
    extraPorts: []
  ## Network Policy configuration
  ## ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param indexNode.networkPolicy.enabled Enable creation of NetworkPolicy resources
    ##
    enabled: true
    ## @param indexNode.networkPolicy.allowExternal The Policy model to apply
    ## When set to false, only pods with the correct client label will have network access to the ports Milvus is
    ## listening on. When true, Milvus will accept connections from any source (with the correct destination port).
    ##
    allowExternal: true
    ## @param indexNode.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param indexNode.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraIngress: []
    ## @param indexNode.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param indexNode.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces
    ## @param indexNode.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces
    ##
    ingressNSMatchLabels: {}
    ingressNSPodMatchLabels: {}
  ## @section Index Node Metrics Parameters
  ## Prometheus Exporter / Metrics
  ##
  metrics:
    ## @param indexNode.metrics.enabled Enable metrics
    ##
    enabled: false
    ## @param indexNode.metrics.annotations [object] Annotations for the server service in order to scrape metrics
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "{{ .Values.indexNode.service.ports.grpc }}"
    ## Prometheus Operator ServiceMonitor configuration
    ##
    serviceMonitor:
      ## @param indexNode.metrics.serviceMonitor.enabled Create ServiceMonitor Resource for scraping metrics using Prometheus Operator
      ##
      enabled: false
      ## @param indexNode.metrics.serviceMonitor.annotations Annotations for the ServiceMonitor Resource
      ##
      annotations: ""
      ## @param indexNode.metrics.serviceMonitor.namespace Namespace for the ServiceMonitor Resource (defaults to the Release Namespace)
      ##
      namespace: ""
      ## @param indexNode.metrics.serviceMonitor.interval Interval at which metrics should be scraped.
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      interval: ""
      ## @param indexNode.metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      scrapeTimeout: ""
      ## @param indexNode.metrics.serviceMonitor.labels Additional labels that can be used so ServiceMonitor will be discovered by Prometheus
      ##
      labels: {}
      ## @param indexNode.metrics.serviceMonitor.selector Prometheus instance selector labels
      ## ref: https://github.com/bitnami/charts/tree/main/bitnami/prometheus-operator#prometheus-configuration
      ##
      selector: {}
      ## @param indexNode.metrics.serviceMonitor.relabelings RelabelConfigs to apply to samples before scraping
      ##
      relabelings: []
      ## @param indexNode.metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion
      ##
      metricRelabelings: []
      ## @param indexNode.metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint
      ##
      honorLabels: false
      ## @param indexNode.metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in prometheus.
      ##
      jobLabel: ""
## @section Proxy Deployment Parameters
##
proxy:
  ## @param proxy.enabled Enable Proxy deployment
  ##
  enabled: true
  ## @section Proxy TLS Connection Configuration Parameters
  ##
  tls:
    ## @param proxy.tls.mode TLS mode for proxy. Allowed values: `0`, `1`, `2`
    ## 0 for close, 1 for one-way authentication, 2 for two-way authentication.
    ##
    mode: 0
    ## @param proxy.tls.existingSecret Name of the existing secret containing the TLS certificates for proxy.
    ## if mode equals to 1, existingSecret should contain a server cert and a server key
    ## if mode equals to 2, existingSecret should contain a server cert, a server key and a CA cert
    ##
    existingSecret: ""
    ## @param proxy.tls.cert The secret key from the existingSecret if 'cert' key different from the default (server.pem)
    ##
    cert: server.pem
    ## @param proxy.tls.key The secret key from the existingSecret if 'key' key different from the default (server.key)
    ## Milvus cannot decrypt key file with password, so now the key must not be encrypted
    ##
    key: server.key
    ## @param proxy.tls.caCert The secret key from the existingSecret if 'caCert' key different from the default (ca.pem)
    ##
    caCert: ca.pem
    ## @param proxy.tls.keyPassword Password to access the password-protected PEM key if necessary.
    ## Milvus cannot decrypt key file with password, so now the key must not be encrypted, value here must be empty
    ##
    keyPassword: ""
  ## @param proxy.extraEnvVars Array with extra environment variables to add to proxy nodes
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  ## @param proxy.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for proxy nodes
  ##
  extraEnvVarsCM: ""
  ## @param proxy.extraEnvVarsSecret Name of existing Secret containing extra env vars for proxy nodes
  ##
  extraEnvVarsSecret: ""
  ## @param proxy.defaultConfig [string] Default override configuration from the common set in milvus.defaultConfig
  ##
  defaultConfig: |
    # Override the port for internal binding (the external components will use the service port defined in milvus.defaultConfig)
    proxy:
      port: {{ .Values.proxy.containerPorts.grpc }}
      internalPort: {{ .Values.proxy.containerPorts.grpcInternal }}
  ## @param proxy.existingConfigMap name of a ConfigMap with existing configuration for the default configuration
  ##
  existingConfigMap: ""
  ## @param proxy.extraConfig Override configuration
  ##
  extraConfig: {}
  ## @param proxy.extraConfigExistingConfigMap name of a ConfigMap with existing configuration for the Dashboard
  ##
  extraConfigExistingConfigMap: ""
  ## @param proxy.command Override default container command (useful when using custom images)
  ##
  command: []
  ## @param proxy.args Override default container args (useful when using custom images)
  ##
  args: []
  ## @param proxy.replicaCount Number of Proxy replicas to deploy
  ##
  replicaCount: 1
  ## @param proxy.containerPorts.grpc GRPC port for Proxy
  ## @param proxy.containerPorts.grpcInternal GRPC internal port for Proxy
  ## @param proxy.containerPorts.metrics Metrics port for Proxy
  containerPorts:
    grpc: 19530
    grpcInternal: 19529
    metrics: 9091
  ## Configure extra options for Proxy containers' liveness, readiness and startup probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
  ## @param proxy.livenessProbe.enabled Enable livenessProbe on Proxy nodes
  ## @param proxy.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param proxy.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param proxy.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param proxy.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param proxy.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param proxy.readinessProbe.enabled Enable readinessProbe on Proxy nodes
  ## @param proxy.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param proxy.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param proxy.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param proxy.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param proxy.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param proxy.startupProbe.enabled Enable startupProbe on Proxy containers
  ## @param proxy.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param proxy.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param proxy.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param proxy.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param proxy.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param proxy.customLivenessProbe Custom livenessProbe that overrides the default one
  ##
  customLivenessProbe: {}
  ## @param proxy.customReadinessProbe Custom readinessProbe that overrides the default one
  ##
  customReadinessProbe: {}
  ## @param proxy.customStartupProbe Custom startupProbe that overrides the default one
  ##
  customStartupProbe: {}
  ## proxy resource requests and limits
  ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param proxy.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if proxy.resources is set (proxy.resources is recommended for production).
  ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
  ##
  resourcesPreset: "micro"
  ## @param proxy.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param proxy.podSecurityContext.enabled Enabled Proxy pods' Security Context
  ## @param proxy.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
  ## @param proxy.podSecurityContext.sysctls Set kernel settings using the sysctl interface
  ## @param proxy.podSecurityContext.supplementalGroups Set filesystem extra groups
  ## @param proxy.podSecurityContext.fsGroup Set Proxy pod's Security Context fsGroup
  ##
  podSecurityContext:
    enabled: true
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## Configure Container Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param proxy.containerSecurityContext.enabled Enabled containers' Security Context
  ## @param proxy.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
  ## @param proxy.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
  ## @param proxy.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
  ## @param proxy.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
  ## @param proxy.containerSecurityContext.privileged Set container's Security Context privileged
  ## @param proxy.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
  ## @param proxy.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
  ## @param proxy.containerSecurityContext.capabilities.drop List of capabilities to be dropped
  ## @param proxy.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
  ##
  containerSecurityContext:
    enabled: true
    seLinuxOptions: {}
    runAsUser: 1001
    runAsGroup: 1001
    runAsNonRoot: true
    privileged: false
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    capabilities:
      drop: ["ALL"]
    seccompProfile:
      type: "RuntimeDefault"
  ## @param proxy.lifecycleHooks for the proxy container(s) to automate configuration before or after startup
  ##
  lifecycleHooks: {}
  ## @param proxy.runtimeClassName Name of the runtime class to be used by pod(s)
  ## ref: https://kubernetes.io/docs/concepts/containers/runtime-class/
  ##
  runtimeClassName: ""
  ## @param proxy.automountServiceAccountToken Mount Service Account token in pod
  ##
  automountServiceAccountToken: false
  ## @param proxy.hostAliases proxy pods host aliases
  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
  ##
  hostAliases: []
  ## @param proxy.podLabels Extra labels for proxy pods
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  ##
  podLabels: {}
  ## @param proxy.podAnnotations Annotations for proxy pods
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  podAnnotations: {}
  ## @param proxy.podAffinityPreset Pod affinity preset. Ignored if `proxy.affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAffinityPreset: ""
  ## @param proxy.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `proxy.affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAntiAffinityPreset: soft
  ## Node proxy.affinity preset
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  ##
  nodeAffinityPreset:
    ## @param proxy.nodeAffinityPreset.type Node affinity preset type. Ignored if `proxy.affinity` is set. Allowed values: `soft` or `hard`
    ##
    type: ""
    ## @param proxy.nodeAffinityPreset.key Node label key to match. Ignored if `proxy.affinity` is set
    ##
    key: ""
    ## @param proxy.nodeAffinityPreset.values Node label values to match. Ignored if `proxy.affinity` is set
    ## E.g.
    ## values:
    ##   - e2e-az1
    ##   - e2e-az2
    ##
    values: []
  ## @param proxy.affinity Affinity for Proxy pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## NOTE: `proxy.podAffinityPreset`, `proxy.podAntiAffinityPreset`, and `proxy.nodeAffinityPreset` will be ignored when it's set
  ##
  affinity: {}
  ## @param proxy.nodeSelector Node labels for Proxy pods assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  ##
  nodeSelector: {}
  ## @param proxy.tolerations Tolerations for Proxy pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param proxy.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
  ##
  topologySpreadConstraints: []
  ## @param proxy.priorityClassName Proxy pods' priorityClassName
  ##
  priorityClassName: ""
  ## @param proxy.schedulerName Kubernetes pod scheduler registry
  ## https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""
  ## @param proxy.updateStrategy.type Proxy statefulset strategy type
  ## @param proxy.updateStrategy.rollingUpdate Proxy statefulset rolling update configuration parameters
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ##
  updateStrategy:
    type: RollingUpdate
    rollingUpdate: {}
  ## @param proxy.extraVolumes Optionally specify extra list of additional volumes for the Proxy pod(s)
  ##
  extraVolumes: []
  ## @param proxy.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the Proxy container(s)
  ##
  extraVolumeMounts: []
  ## @param proxy.sidecars Add additional sidecar containers to the Proxy pod(s)
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## @param proxy.enableDefaultInitContainers Deploy default init containers
  ##
  enableDefaultInitContainers: true
  ## @param proxy.initContainers Add additional init containers to the Proxy pod(s)
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
  ## e.g:
  ## initContainers:
  ##  - name: your-image-name
  ##    image: your-image
  ##    imagePullPolicy: Always
  ##    command: ['sh', '-c', 'echo "hello world"']
  ##
  initContainers: []
  ## Service account for Proxy to use
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param proxy.serviceAccount.create Enable creation of ServiceAccount for Proxy pods
    ##
    create: true
    ## @param proxy.serviceAccount.name The name of the ServiceAccount to use
    ## If not set and create is true, a name is generated using the common.names.fullname template
    ##
    name: ""
    ## @param proxy.serviceAccount.automountServiceAccountToken Allows auto mount of ServiceAccountToken on the serviceAccount created
    ## Can be set to false if pods using this serviceAccount do not need to use K8s API
    ##
    automountServiceAccountToken: false
    ## @param proxy.serviceAccount.annotations Additional custom annotations for the ServiceAccount
    ##
    annotations: {}
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
  ## @param proxy.pdb.create Enable/disable a Pod Disruption Budget creation
  ## @param proxy.pdb.minAvailable [object] Minimum number/percentage of pods that should remain scheduled
  ## @param proxy.pdb.maxUnavailable [object] Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `proxy.pdb.minAvailable` and `proxy.pdb.maxUnavailable` are empty.
  ##
  pdb:
    create: true
    minAvailable: ""
    maxUnavailable: ""
  ## @section Proxy Autoscaling configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ##
  autoscaling:
    vpa:
      ## @param proxy.autoscaling.vpa.enabled Enable VPA
      ##
      enabled: false
      ## @param proxy.autoscaling.vpa.annotations Annotations for VPA resource
      ##
      annotations: {}
      ## @param proxy.autoscaling.vpa.controlledResources VPA List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
      ##
      controlledResources: []
      ## @param proxy.autoscaling.vpa.maxAllowed VPA Max allowed resources for the pod
      ## cpu: 200m
      ## memory: 100Mi
      maxAllowed: {}
      ## @param proxy.autoscaling.vpa.minAllowed VPA Min allowed resources for the pod
      ## cpu: 200m
      ## memory: 100Mi
      minAllowed: {}
      updatePolicy:
        ## @param proxy.autoscaling.vpa.updatePolicy.updateMode Autoscaling update policy Specifies whether recommended updates are applied when a Pod is started and whether recommended updates are applied during the life of a Pod
        ## Possible values are "Off", "Initial", "Recreate", and "Auto".
        ##
        updateMode: Auto
    hpa:
      ## @param proxy.autoscaling.hpa.enabled Enable HPA for Milvus Data Plane
      ##
      enabled: false
      ## @param proxy.autoscaling.hpa.annotations Annotations for HPA resource
      ##
      annotations: {}
      ## @param proxy.autoscaling.hpa.minReplicas Minimum number of Milvus Data Plane replicas
      ##
      minReplicas: ""
      ## @param proxy.autoscaling.hpa.maxReplicas Maximum number of Milvus Data Plane replicas
      ##
      maxReplicas: ""
      ## @param proxy.autoscaling.hpa.targetCPU Target CPU utilization percentage
      ##
      targetCPU: ""
      ## @param proxy.autoscaling.hpa.targetMemory Target Memory utilization percentage
      ##
      targetMemory: ""
  ## @section Proxy Traffic Exposure Parameters
  ##

  ## proxy service parameters
  ##
  service:
    ## @param proxy.service.type Proxy service type
    ##
    type: LoadBalancer
    ## @param proxy.service.ports.grpc Proxy GRPC service port
    ## @param proxy.service.ports.metrics Proxy Metrics service port
    ##
    ports:
      grpc: 19530
      metrics: 9091
    ## Node ports to expose
    ## NOTE: choose port between <30000-32767>
    ## @param proxy.service.nodePorts.grpc Node port for GRPC
    ## @param proxy.service.nodePorts.metrics Node port for Metrics
    ##
    nodePorts:
      grpc: ""
      metrics: ""
    ## @param proxy.service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}
    ## @param proxy.service.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param proxy.service.clusterIP Proxy service Cluster IP
    ## e.g.:
    ## clusterIP: None
    ##
    clusterIP: ""
    ## @param proxy.service.loadBalancerIP Proxy service Load Balancer IP
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
    ##
    loadBalancerIP: ""
    ## @param proxy.service.loadBalancerSourceRanges Proxy service Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## e.g:
    ## loadBalancerSourceRanges:
    ##   - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## @param proxy.service.externalTrafficPolicy Proxy service external traffic policy
    ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param proxy.service.annotations Additional custom annotations for Proxy service
    ##
    annotations: {}
    ## @param proxy.service.extraPorts Extra ports to expose in the Proxy service
    ##
    extraPorts: []
  ## Network Policy configuration
  ## ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param proxy.networkPolicy.enabled Enable creation of NetworkPolicy resources
    ##
    enabled: true
    ## @param proxy.networkPolicy.allowExternal The Policy model to apply
    ## When set to false, only pods with the correct client label will have network access to the ports Milvus is
    ## listening on. When true, Milvus will accept connections from any source (with the correct destination port).
    ##
    allowExternal: true
    ## @param proxy.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param proxy.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraIngress: []
    ## @param proxy.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param proxy.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces
    ## @param proxy.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces
    ##
    ingressNSMatchLabels: {}
    ingressNSPodMatchLabels: {}
  ## @section Proxy Metrics Parameters
  ## Prometheus Exporter / Metrics
  ##
  metrics:
    ## @param proxy.metrics.enabled Enable metrics
    ##
    enabled: false
    ## @param proxy.metrics.annotations [object] Annotations for the server service in order to scrape metrics
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "{{ .Values.proxy.service.ports.grpc }}"
    ## Prometheus Operator ServiceMonitor configuration
    ##
    serviceMonitor:
      ## @param proxy.metrics.serviceMonitor.enabled Create ServiceMonitor Resource for scraping metrics using Prometheus Operator
      ##
      enabled: false
      ## @param proxy.metrics.serviceMonitor.annotations Annotations for the ServiceMonitor Resource
      ##
      annotations: ""
      ## @param proxy.metrics.serviceMonitor.namespace Namespace for the ServiceMonitor Resource (defaults to the Release Namespace)
      ##
      namespace: ""
      ## @param proxy.metrics.serviceMonitor.interval Interval at which metrics should be scraped.
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      interval: ""
      ## @param proxy.metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      scrapeTimeout: ""
      ## @param proxy.metrics.serviceMonitor.labels Additional labels that can be used so ServiceMonitor will be discovered by Prometheus
      ##
      labels: {}
      ## @param proxy.metrics.serviceMonitor.selector Prometheus instance selector labels
      ## ref: https://github.com/bitnami/charts/tree/main/bitnami/prometheus-operator#prometheus-configuration
      ##
      selector: {}
      ## @param proxy.metrics.serviceMonitor.relabelings RelabelConfigs to apply to samples before scraping
      ##
      relabelings: []
      ## @param proxy.metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion
      ##
      metricRelabelings: []
      ## @param proxy.metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint
      ##
      honorLabels: false
      ## @param proxy.metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in prometheus.
      ##
      jobLabel: ""
## @section Attu Deployment Parameters
##
attu:
  ## @param attu.enabled Enable Attu deployment
  ##
  enabled: true
  ## Bitnami Attu image
  ## ref: https://hub.docker.com/r/bitnami/attu/tags/
  ## @param attu.image.registry [default: REGISTRY_NAME] Attu image registry
  ## @param attu.image.repository [default: REPOSITORY_NAME/attu] Attu image repository
  ## @skip attu.image.tag Attu image tag (immutable tags are recommended)
  ## @param attu.image.digest Attu image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
  ## @param attu.image.pullPolicy Attu image pull policy
  ## @param attu.image.pullSecrets Attu image pull secrets
  ## @param attu.image.debug Enable debug mode
  ##
  image:
    registry: docker.io
    repository: bitnami/attu
    tag: 2.5.0-debian-12-r0
    digest: ""
    ## Specify a imagePullPolicy
    ## ref: https://kubernetes.io/docs/concepts/containers/images/#pre-pulled-images
    ##
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## e.g:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []
    ## Enable debug mode
    ##
    debug: false
  ## @param attu.extraEnvVars Array with extra environment variables to add to attu nodes
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  ## @param attu.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for attu nodes
  ##
  extraEnvVarsCM: ""
  ## @param attu.extraEnvVarsSecret Name of existing Secret containing extra env vars for attu nodes
  ##
  extraEnvVarsSecret: ""
  ## @param attu.command Override default container command (useful when using custom images)
  ##
  command: []
  ## @param attu.args Override default container args (useful when using custom images)
  ##
  args: []
  ## @param attu.replicaCount Number of Attu replicas to deploy
  ##
  replicaCount: 1
  ## @param attu.containerPorts.http HTTP port for Attu
  containerPorts:
    http: 3000
  ## Configure extra options for Attu containers' liveness, readiness and startup probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
  ## @param attu.livenessProbe.enabled Enable livenessProbe on Attu nodes
  ## @param attu.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param attu.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param attu.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param attu.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param attu.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param attu.readinessProbe.enabled Enable readinessProbe on Attu nodes
  ## @param attu.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param attu.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param attu.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param attu.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param attu.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param attu.startupProbe.enabled Enable startupProbe on Attu containers
  ## @param attu.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param attu.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param attu.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param attu.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param attu.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 5
    successThreshold: 1
  ## @param attu.customLivenessProbe Custom livenessProbe that overrides the default one
  ##
  customLivenessProbe: {}
  ## @param attu.customReadinessProbe Custom readinessProbe that overrides the default one
  ##
  customReadinessProbe: {}
  ## @param attu.customStartupProbe Custom startupProbe that overrides the default one
  ##
  customStartupProbe: {}
  ## attu resource requests and limits
  ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param attu.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if attu.resources is set (attu.resources is recommended for production).
  ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
  ##
  resourcesPreset: "micro"
  ## @param attu.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param attu.podSecurityContext.enabled Enabled Attu pods' Security Context
  ## @param attu.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
  ## @param attu.podSecurityContext.sysctls Set kernel settings using the sysctl interface
  ## @param attu.podSecurityContext.supplementalGroups Set filesystem extra groups
  ## @param attu.podSecurityContext.fsGroup Set Attu pod's Security Context fsGroup
  ##
  podSecurityContext:
    enabled: true
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## Configure Container Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param attu.containerSecurityContext.enabled Enabled containers' Security Context
  ## @param attu.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
  ## @param attu.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
  ## @param attu.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
  ## @param attu.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
  ## @param attu.containerSecurityContext.privileged Set container's Security Context privileged
  ## @param attu.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
  ## @param attu.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
  ## @param attu.containerSecurityContext.capabilities.drop List of capabilities to be dropped
  ## @param attu.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
  ##
  containerSecurityContext:
    enabled: true
    seLinuxOptions: {}
    runAsUser: 1001
    runAsGroup: 1001
    runAsNonRoot: true
    privileged: false
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    capabilities:
      drop: ["ALL"]
    seccompProfile:
      type: "RuntimeDefault"
  ## @param attu.lifecycleHooks for the attu container(s) to automate configuration before or after startup
  ##
  lifecycleHooks: {}
  ## @param attu.runtimeClassName Name of the runtime class to be used by pod(s)
  ## ref: https://kubernetes.io/docs/concepts/containers/runtime-class/
  ##
  runtimeClassName: ""
  ## @param attu.automountServiceAccountToken Mount Service Account token in pod
  ##
  automountServiceAccountToken: false
  ## @param attu.hostAliases attu pods host aliases
  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
  ##
  hostAliases: []
  ## @param attu.podLabels Extra labels for attu pods
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  ##
  podLabels: {}
  ## @param attu.podAnnotations Annotations for attu pods
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  podAnnotations: {}
  ## @param attu.podAffinityPreset Pod affinity preset. Ignored if `attu.affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAffinityPreset: ""
  ## @param attu.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `attu.affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAntiAffinityPreset: soft
  ## Node attu.affinity preset
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  ##
  nodeAffinityPreset:
    ## @param attu.nodeAffinityPreset.type Node affinity preset type. Ignored if `attu.affinity` is set. Allowed values: `soft` or `hard`
    ##
    type: ""
    ## @param attu.nodeAffinityPreset.key Node label key to match. Ignored if `attu.affinity` is set
    ##
    key: ""
    ## @param attu.nodeAffinityPreset.values Node label values to match. Ignored if `attu.affinity` is set
    ## E.g.
    ## values:
    ##   - e2e-az1
    ##   - e2e-az2
    ##
    values: []
  ## @param attu.affinity Affinity for Attu pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## NOTE: `attu.podAffinityPreset`, `attu.podAntiAffinityPreset`, and `attu.nodeAffinityPreset` will be ignored when it's set
  ##
  affinity: {}
  ## @param attu.nodeSelector Node labels for Attu pods assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  ##
  nodeSelector: {}
  ## @param attu.tolerations Tolerations for Attu pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param attu.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
  ##
  topologySpreadConstraints: []
  ## @param attu.priorityClassName Attu pods' priorityClassName
  ##
  priorityClassName: ""
  ## @param attu.schedulerName Kubernetes pod scheduler registry
  ## https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""
  ## @param attu.updateStrategy.type Attu statefulset strategy type
  ## @param attu.updateStrategy.rollingUpdate Attu statefulset rolling update configuration parameters
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ##
  updateStrategy:
    type: RollingUpdate
    rollingUpdate: {}
  ## @param attu.extraVolumes Optionally specify extra list of additional volumes for the Attu pod(s)
  ##
  extraVolumes: []
  ## @param attu.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the Attu container(s)
  ##
  extraVolumeMounts: []
  ## @param attu.sidecars Add additional sidecar containers to the Attu pod(s)
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## @param attu.enableDefaultInitContainers Deploy default init containers
  ##
  enableDefaultInitContainers: true
  ## @param attu.initContainers Add additional init containers to the Attu pod(s)
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
  ## e.g:
  ## initContainers:
  ##  - name: your-image-name
  ##    image: your-image
  ##    imagePullPolicy: Always
  ##    command: ['sh', '-c', 'echo "hello world"']
  ##
  initContainers: []
  ## Service account for Attu to use
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param attu.serviceAccount.create Enable creation of ServiceAccount for Attu pods
    ##
    create: true
    ## @param attu.serviceAccount.name The name of the ServiceAccount to use
    ## If not set and create is true, a name is generated using the common.names.fullname template
    ##
    name: ""
    ## @param attu.serviceAccount.automountServiceAccountToken Allows auto mount of ServiceAccountToken on the serviceAccount created
    ## Can be set to false if pods using this serviceAccount do not need to use K8s API
    ##
    automountServiceAccountToken: false
    ## @param attu.serviceAccount.annotations Additional custom annotations for the ServiceAccount
    ##
    annotations: {}
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
  ## @param attu.pdb.create Enable/disable a Pod Disruption Budget creation
  ## @param attu.pdb.minAvailable [object] Minimum number/percentage of pods that should remain scheduled
  ## @param attu.pdb.maxUnavailable [object] Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `attu.pdb.minAvailable` and `attu.pdb.maxUnavailable` are empty.
  ##
  pdb:
    create: true
    minAvailable: ""
    maxUnavailable: ""
  ## @section Attu Autoscaling configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ##
  autoscaling:
    vpa:
      ## @param attu.autoscaling.vpa.enabled Enable VPA
      ##
      enabled: false
      ## @param attu.autoscaling.vpa.annotations Annotations for VPA resource
      ##
      annotations: {}
      ## @param attu.autoscaling.vpa.controlledResources VPA List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
      ##
      controlledResources: []
      ## @param attu.autoscaling.vpa.maxAllowed VPA Max allowed resources for the pod
      ## cpu: 200m
      ## memory: 100Mi
      maxAllowed: {}
      ## @param attu.autoscaling.vpa.minAllowed VPA Min allowed resources for the pod
      ## cpu: 200m
      ## memory: 100Mi
      minAllowed: {}
      updatePolicy:
        ## @param attu.autoscaling.vpa.updatePolicy.updateMode Autoscaling update policy Specifies whether recommended updates are applied when a Pod is started and whether recommended updates are applied during the life of a Pod
        ## Possible values are "Off", "Initial", "Recreate", and "Auto".
        ##
        updateMode: Auto
    hpa:
      ## @param attu.autoscaling.hpa.enabled Enable HPA for Milvus Data Plane
      ##
      enabled: false
      ## @param attu.autoscaling.hpa.annotations Annotations for HPA resource
      ##
      annotations: {}
      ## @param attu.autoscaling.hpa.minReplicas Minimum number of Milvus Data Plane replicas
      ##
      minReplicas: ""
      ## @param attu.autoscaling.hpa.maxReplicas Maximum number of Milvus Data Plane replicas
      ##
      maxReplicas: ""
      ## @param attu.autoscaling.hpa.targetCPU Target CPU utilization percentage
      ##
      targetCPU: ""
      ## @param attu.autoscaling.hpa.targetMemory Target Memory utilization percentage
      ##
      targetMemory: ""
  ## @section Attu Traffic Exposure Parameters
  ##

  ## attu service parameters
  ##
  service:
    ## @param attu.service.type Attu service type
    ##
    type: LoadBalancer
    ## @param attu.service.ports.http Attu HTTP service port
    ##
    ports:
      http: 80
    ## Node ports to expose
    ## NOTE: choose port between <30000-32767>
    ## @param attu.service.nodePorts.http Node port for HTTP
    ##
    nodePorts:
      http: ""
    ## @param attu.service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}
    ## @param attu.service.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param attu.service.clusterIP Attu service Cluster IP
    ## e.g.:
    ## clusterIP: None
    ##
    clusterIP: ""
    ## @param attu.service.loadBalancerIP Attu service Load Balancer IP
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
    ##
    loadBalancerIP: ""
    ## @param attu.service.loadBalancerSourceRanges Attu service Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## e.g:
    ## loadBalancerSourceRanges:
    ##   - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## @param attu.service.externalTrafficPolicy Attu service external traffic policy
    ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param attu.service.annotations Additional custom annotations for Attu service
    ##
    annotations: {}
    ## @param attu.service.extraPorts Extra ports to expose in the Attu service
    ##
    extraPorts: []
  ## ref: http://kubernetes.io/docs/concepts/services-networking/ingress/
  ##
  ingress:
    ## @param attu.ingress.enabled Enable ingress record generation for Milvus
    ##
    enabled: false
    ## @param attu.ingress.pathType Ingress path type
    ##
    pathType: ImplementationSpecific
    ## @param attu.ingress.apiVersion Force Ingress API version (automatically detected if not set)
    ##
    apiVersion: ""
    ## @param attu.ingress.hostname Default host for the ingress record
    ##
    hostname: milvus.local
    ## @param attu.ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
    ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
    ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
    ##
    ingressClassName: ""
    ## @param attu.ingress.path Default path for the ingress record
    ## NOTE: You may need to set this to '/*' in order to use this with ALB ingress controllers
    ##
    path: /
    ## @param attu.ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
    ## Use this parameter to set the required annotations for cert-manager, see
    ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
    ## e.g:
    ## annotations:
    ##   kubernetes.io/ingress.class: nginx
    ##   cert-manager.io/cluster-issuer: cluster-issuer-name
    ##
    annotations: {}
    ## @param attu.ingress.tls Enable TLS configuration for the host defined at `attu.ingress.hostname` parameter
    ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.ingress.hostname }}`
    ## You can:
    ##   - Use the `attu.ingress.secrets` parameter to create this TLS secret
    ##   - Rely on cert-manager to create it by setting the corresponding annotations
    ##   - Rely on Helm to create self-signed certificates by setting `attu.ingress.selfSigned=true`
    ##
    tls: false
    ## @param attu.ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
    ##
    selfSigned: false
    ## @param attu.ingress.extraHosts An array with additional hostname(s) to be covered with the ingress record
    ## e.g:
    ## extraHosts:
    ##   - name: milvus.local
    ##     path: /
    ##
    extraHosts: []
    ## @param attu.ingress.extraPaths An array with additional arbitrary paths that may need to be added to the ingress under the main host
    ## e.g:
    ## extraPaths:
    ## - path: /*
    ##   backend:
    ##     serviceName: ssl-redirect
    ##     servicePort: use-annotation
    ##
    extraPaths: []
    ## @param attu.ingress.extraTls TLS configuration for additional hostname(s) to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
    ## e.g:
    ## extraTls:
    ## - hosts:
    ##     - milvus.local
    ##   secretName: milvus.local-tls
    ##
    extraTls: []
    ## @param attu.ingress.secrets Custom TLS certificates as secrets
    ## NOTE: 'key' and 'certificate' are expected in PEM format
    ## NOTE: 'name' should line up with a 'secretName' set further up
    ## If it is not set and you're using cert-manager, this is unneeded, as it will create a secret for you with valid certificates
    ## If it is not set and you're NOT using cert-manager either, self-signed certificates will be created valid for 365 days
    ## It is also possible to create and manage the certificates outside of this helm chart
    ## Please see README.md for more information
    ## e.g:
    ## secrets:
    ##   - name: milvus.local-tls
    ##     key: |-
    ##       -----BEGIN RSA PRIVATE KEY-----
    ##       ...
    ##       -----END RSA PRIVATE KEY-----
    ##     certificate: |-
    ##       -----BEGIN CERTIFICATE-----
    ##       ...
    ##       -----END CERTIFICATE-----
    ##
    secrets: []
    ## @param attu.ingress.extraRules Additional rules to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
    ## e.g:
    ## extraRules:
    ## - host: example.local
    ##     http:
    ##       path: /
    ##       backend:
    ##         service:
    ##           name: example-svc
    ##           port:
    ##             name: http
    ##
    extraRules: []
  ## Network Policy configuration
  ## ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param attu.networkPolicy.enabled Enable creation of NetworkPolicy resources
    ##
    enabled: true
    ## @param attu.networkPolicy.allowExternal The Policy model to apply
    ## When set to false, only pods with the correct client label will have network access to the ports Milvus is
    ## listening on. When true, Milvus will accept connections from any source (with the correct destination port).
    ##
    allowExternal: true
    ## @param attu.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param attu.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraIngress: []
    ## @param attu.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param attu.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces
    ## @param attu.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces
    ##
    ingressNSMatchLabels: {}
    ingressNSPodMatchLabels: {}
## @section Init Container Parameters
##
waitContainer:
  ## @param waitContainer.image.registry [default: REGISTRY_NAME] Init container wait-container image registry
  ## @param waitContainer.image.repository [default: REPOSITORY_NAME/os-shell] Init container wait-container image name
  ## @skip waitContainer.image.tag Init container wait-container image tag
  ## @param waitContainer.image.digest Init container wait-container image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
  ##
  image:
    registry: docker.io
    repository: bitnami/os-shell
    tag: 12-debian-12-r34
    digest: ""
    ## @param waitContainer.image.pullPolicy Init container wait-container image pull policy
    ##
    pullPolicy: IfNotPresent
    ## @param waitContainer.image.pullSecrets [array] Specify docker-registry secret names as an array
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## e.g:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []
  ## Configure Container Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
  ## @param waitContainer.containerSecurityContext.enabled Enabled containers' Security Context
  ## @param waitContainer.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
  ## @param waitContainer.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
  ## @param waitContainer.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
  ## @param waitContainer.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
  ## @param waitContainer.containerSecurityContext.privileged Set container's Security Context privileged
  ## @param waitContainer.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
  ## @param waitContainer.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
  ## @param waitContainer.containerSecurityContext.capabilities.drop List of capabilities to be dropped
  ## @param waitContainer.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
  ##
  containerSecurityContext:
    enabled: true
    seLinuxOptions: {}
    runAsUser: 1001
    runAsGroup: 1001
    runAsNonRoot: true
    privileged: false
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    capabilities:
      drop: ["ALL"]
    seccompProfile:
      type: "RuntimeDefault"
  ## Container resource requests and limits
  ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param waitContainer.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, small, medium, large, xlarge, 2xlarge). This is ignored if initJob.resources is set (initJob.resources is recommended for production).
  ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
  ##
  resourcesPreset: "micro"
  ## @param waitContainer.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
## @section External etcd settings
##
externalEtcd:
  ## @param externalEtcd.servers List of hostnames of the external etcd
  ##
  servers: []
  ## @param externalEtcd.port Port of the external etcd instance
  ##
  port: 2379
  ## @param externalEtcd.user User of the external etcd instance
  ##
  user: root
  ## @param externalEtcd.password Password of the external etcd instance
  ##
  password: ""
  ## @param externalEtcd.existingSecret Name of a secret containing the external etcd password
  ##
  existingSecret: ""
  ## @param externalEtcd.existingSecretPasswordKey Key inside the secret containing the external etcd password
  ##
  existingSecretPasswordKey: "etcd-root-password"
  ## External etcd TLS connection configuration
  ##
  tls:
    ## @param externalEtcd.tls.enabled Enable TLS for etcd client connections.
    ##
    enabled: false
    ## @param externalEtcd.tls.existingSecret Name of the existing secret containing the TLS certificates for external etcd client communications.
    ##
    existingSecret: ""
    ## @param externalEtcd.tls.cert The secret key from the existingSecret if 'cert' key different from the default (tls.crt)
    ##
    cert: tls.crt
    ## @param externalEtcd.tls.key The secret key from the existingSecret if 'key' key different from the default (tls.key)
    ## Milvus cannot decrypt key file with password, so now the key must not be encrypted
    ##
    key: tls.key
    ## @param externalEtcd.tls.caCert The secret key from the existingSecret if 'caCert' key different from the default (ca.crt)
    ##
    caCert: ca.crt
    ## @param externalEtcd.tls.keyPassword Password to access the password-protected PEM key if necessary.
    ## Milvus cannot decrypt key file with password, so now the key must not be encrypted, value here must be empty
    ##
    keyPassword: ""
## @section External S3 parameters
## All of these values are only used when minio.enabled is set to false
## @param externalS3.host External S3 host
## @param externalS3.port External S3 port number
## @param externalS3.accessKeyID External S3 access key ID
## @param externalS3.accessKeySecret External S3 access key secret
## @param externalS3.existingSecret Name of an existing secret resource containing the S3 credentials
## @param externalS3.existingSecretAccessKeyIDKey Name of an existing secret key containing the S3 access key ID
## @param externalS3.existingSecretKeySecretKey Name of an existing secret key containing the S3 access key secret
## @param externalS3.bucket External S3 bucket
## @param externalS3.rootPath External S3 root path
## @param externalS3.iamEndpoint External S3 IAM endpoint
## @param externalS3.cloudProvider External S3 cloud provider
##
externalS3:
  host: ""
  port: 443
  accessKeyID: ""
  accessKeySecret: ""
  existingSecret: ""
  existingSecretAccessKeyIDKey: "root-user"
  existingSecretKeySecretKey: "root-password"
  ## External S3 TLS connection configuration
  ##
  tls:
    ## @param externalS3.tls.enabled Enable TLS for externalS3 client connections.
    ##
    enabled: false
    ## @param externalS3.tls.existingSecret Name of the existing secret containing the TLS certificates for externalS3 client communications.
    ##
    existingSecret: ""
    ## @param externalS3.tls.caCert The secret key from the existingSecret if 'caCert' key different from the default (ca.crt)
    ##
    caCert: ca.crt
  bucket: "milvus"
  rootPath: "file"
  iamEndpoint: ""
  cloudProvider: ""
## @section External Kafka parameters
## All of these values are ignored when kafka.enabled is set to true
##
externalKafka:
  ## @param externalKafka.servers External Kafka brokers
  ## Multiple brokers can be provided in a comma separated list, e.g. host1:port1,host2:port2
  ##
  servers:
    - localhost
  ## @param externalKafka.port External Kafka port
  ##
  port: 9092
  ## @param externalKafka.listener.protocol Kafka listener protocol. Allowed protocols: PLAINTEXT, SASL_PLAINTEXT, SASL_SSL and SSL
  ##
  listener:
    protocol: PLAINTEXT
  ## Authentication parameters
  ## @param externalKafka.sasl.user User for SASL authentication
  ## @param externalKafka.sasl.password Password for SASL authentication
  ## @param externalKafka.sasl.existingSecret Name of the existing secret containing a password for SASL authentication (under the key named "client-passwords")
  ## @param externalKafka.sasl.existingSecretPasswordKey Name of the secret key containing the Kafka client user password
  ## @param externalKafka.sasl.enabledMechanisms Kafka enabled SASL mechanisms
  ##
  sasl:
    user: user
    password: ""
    existingSecret: ""
    existingSecretPasswordKey: "kafka-root-password"
    enabledMechanisms: "PLAIN"
  ## External kafka TLS connection configuration
  ##
  tls:
    ## @param externalKafka.tls.enabled Enable TLS for kafka client connections.
    ##
    enabled: false
    ## @param externalKafka.tls.existingSecret Name of the existing secret containing the TLS certificates for external kafka client communications.
    ##
    existingSecret: ""
    ## @param externalKafka.tls.cert The secret key from the existingSecret if 'cert' key different from the default (tls.crt)
    ##
    cert: tls.crt
    ## @param externalKafka.tls.key The secret key from the existingSecret if 'key' key different from the default (tls.key)
    ##
    key: tls.key
    ## @param externalKafka.tls.caCert The secret key from the existingSecret if 'caCert' key different from the default (ca.crt)
    ##
    caCert: ca.crt
    ## @param externalKafka.tls.keyPassword Password to access the password-protected PEM key if necessary.
    ##
    keyPassword: ""


## @section etcd sub-chart parameters
##
etcd:
  ## @param etcd.enabled Deploy etcd sub-chart
  ##
  enabled: true
  ## @param etcd.replicaCount Number of etcd replicas
  ##
  replicaCount: 3
  ## @param etcd.containerPorts.client Container port for etcd
  ##
  containerPorts:
    client: 2379
  ## @param etcd.auth.rbac.create Switch to enable RBAC authentication
  ## @param etcd.auth.client.secureTransport use TLS for client-to-server communications
  ##
  auth:
    rbac:
      # Milvus does not have support for etcd authentication
      # https://github.com/milvus-io/milvus/blob/master/pkg/util/paramtable/service_param.go#L93
      create: false
    client:
      secureTransport: false
## @section MinIO&reg; chart parameters
## @extra minio For full list of MinIO&reg; values configurations please refere [here](https://github.com/bitnami/charts/tree/main/bitnami/minio)
##
minio:
  ## @param minio.enabled Enable/disable MinIO&reg; chart installation
  ## to be used as an objstore for Mastodon
  ##
  enabled: true
  ## MinIO&reg; authentication parameters
  ##
  auth:
    ## @param minio.auth.rootUser MinIO&reg; root username
    ##
    rootUser: admin
    ## @param minio.auth.rootPassword Password for MinIO&reg; root user
    ##
    rootPassword: ""
    ## @param minio.auth.existingSecret Name of an existing secret containing the MinIO&reg; credentials
    ##
    existingSecret: ""
  ## @param minio.defaultBuckets Comma, semi-colon or space separated list of MinIO&reg; buckets to create
  ##
  defaultBuckets: "milvus"
  ## @param minio.provisioning.enabled Enable/disable MinIO&reg; provisioning job
  ## @param minio.provisioning.extraCommands Extra commands to run on MinIO&reg; provisioning job
  ##
  provisioning:
    enabled: true
    # We need to allow downloads in order for the UI to work
    extraCommands: ["mc anonymous set download provisioning/milvus"]
  ## @param minio.tls.enabled Enable/disable MinIO&reg; TLS support
  ##
  tls:
    enabled: false
  ## @param minio.service.type MinIO&reg; service type
  ## @param minio.service.loadBalancerIP MinIO&reg; service LoadBalancer IP
  ## @param minio.service.ports.api MinIO&reg; service port
  ##
  service:
    type: ClusterIP
    loadBalancerIP: ""
    ports:
      api: 80
## @section kafka sub-chart paramaters
## https://github.com/bitnami/charts/blob/main/bitnami/kafka/values.yaml
##
kafka:
  ## @param kafka.enabled Enable/disable Kafka chart installation
  ##
  enabled: true
  ## @param kafka.controller.replicaCount Number of Kafka controller eligible (controller+broker) nodes
  ##
  controller:
    replicaCount: 1
  ## @param kafka.service.ports.client Kafka svc port for client connections
  ##
  service:
    ports:
      client: 9092
  ## @param kafka.extraConfig Additional configuration to be appended at the end of the generated Kafka configuration file.
  ##
  extraConfig: |-
    offsets.topic.replication.factor=1
  ## @param kafka.listeners.client.protocol Kafka authentication protocol for the client listener
  ##
  listeners:
    client:
      protocol: SASL_PLAINTEXT
  sasl:
    ## @param kafka.sasl.enabledMechanisms Kafka enabled SASL mechanisms
    ##
    enabledMechanisms: "PLAIN"
    ## @param kafka.sasl.client.users Kafka client users
    ##
    client:
      users:
        - user
