CHART NAME: {{ .Chart.Name }}
CHART VERSION: {{ .Chart.Version }}
APP VERSION: {{ .Chart.AppVersion }}

Did you know there are enterprise versions of the Bitnami catalog? For enhanced secure software supply chain features, unlimited pulls from Docker, LTS support, or application customization, see Bitnami Premium or Tanzu Application Catalog. See https://www.arrow.com/globalecs/na/vendors/bitnami for more information.

** Please be patient while the chart is being deployed **

{{- if .Values.diagnosticMode.enabled }}
The chart has been deployed in diagnostic mode. All probes have been disabled and the command has been overwritten with:

  command: {{- include "common.tplvalues.render" (dict "value" .Values.diagnosticMode.command "context" $) | nindent 4 }}
  args: {{- include "common.tplvalues.render" (dict "value" .Values.diagnosticMode.args "context" $) | nindent 4 }}

Get the list of pods by executing:

  kubectl get pods --namespace {{ include "common.names.namespace" . }} -l app.kubernetes.io/instance={{ .Release.Name }}

Access the pod you want to debug by executing

  kubectl exec --namespace {{ include "common.names.namespace" . }} -ti <NAME OF THE POD> -- bash

{{- else }}
{{- if .Values.client.enabled }}
{{- if .Values.source.launchCommand }}
The following command will be executed:

  {{- include "common.tplvalues.render" (dict "value" .Values.source.launchCommand "context" $) | nindent 2 }}

You can see the logs of each running node with:
    kubectl logs [POD_NAME]

and the list of pods:
    kubectl get pods --namespace {{ include "common.names.namespace" . }} -l "app.kubernetes.io/name={{ include "common.names.name" . }},app.kubernetes.io/instance={{ .Release.Name }}"
{{- else }}
You didn't specify any entrypoint to your code.
To run it, you can either deploy again using the `source.launchCommand` option to specify your entrypoint, or execute it manually by jumping into the pods:

1. Get the running pods
    kubectl get pods --namespace {{ include "common.names.namespace" . }} -l "app.kubernetes.io/name={{ include "common.names.name" . }},app.kubernetes.io/instance={{ .Release.Name }}"

2. Get into a pod
    kubectl exec -ti [POD_NAME] bash

3. Execute your script as you would normally do.
{{- end }}

The worker nodes have been deployed and are accessible as follows:

From inside the cluster:

      {{- $workers := .Values.worker.replicaCount | int }}
      {{- range $i, $e := until $workers }}
        {{ include "deepspeed.v0.worker.fullname" $ }}-{{ $i }}.{{ printf "%s-headless" (include "deepspeed.v0.worker.fullname" $) }}:{{ $.Values.worker.containerPorts.ssh }}
      {{- end }}

{{- if and .Values.worker.externalAccess.enabled (eq .Values.worker.externalAccess.service.type "LoadBalancer") }}

From outside the cluster:

NOTE: It may take a few minutes for the LoadBalancer IPs to be available.
        Watch the status with: 'kubectl get svc --namespace {{ include "common.names.namespace" . }} -l "app.kubernetes.io/instance={{ .Release.Name }},app.kubernetes.io/component=worker,pod" -w'

    Worker domain: You will have a different external IP for each Kafka broker. You can get the list of external IPs using the command below:

        echo "$(kubectl get svc --namespace {{ include "common.names.namespace" . }} -l "app.kubernetes.io/instance={{ .Release.Name }},app.kubernetes.io/component=worker,pod" -o jsonpath='{.items[*].status.loadBalancer.ingress[0].ip}' | tr ' ' '\n')"

    Worker port: {{ .Values.worker.externalAccess.service.ports.ssh }}

{{- end }}

For accessing via ssh get the SSH private key as follows:

kubectl get secret --namespace {{ include "common.names.namespace" . }} {{ include "deepspeed.v0.ssh.keySecretName" . }} -o jsonpath="{.data.id_rsa}" | base64 -d > id_rsa

Then configure your SSH client to use that key.
{{- end }}
{{- end }}
{{ include "deepspeed.v0.validateValues" . }}
{{ include "deepspeed.v0.checkRollingTags" . }}
{{- include "common.warnings.resources" (dict "sections" (list "client" "volumePermissions" "worker") "context" $) }}
{{- include "common.warnings.modifiedImages" (dict "images" (list .Values.image .Values.gitImage .Values.volumePermissions.image) "context" $) }}
{{- include "common.errors.insecureImages" (dict "images" (list .Values.image .Values.gitImage .Values.volumePermissions.image) "context" $) }}
