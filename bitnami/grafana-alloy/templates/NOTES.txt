CHART NAME: {{ .Chart.Name }}
CHART VERSION: {{ .Chart.Version }}
APP VERSION: {{ .Chart.AppVersion }}

Did you know there are enterprise versions of the Bitnami catalog? For enhanced secure software supply chain features, unlimited pulls from Docker, LTS support, or application customization, see Bitnami Premium or Tanzu Application Catalog. See https://www.arrow.com/globalecs/na/vendors/bitnami for more information.

** Please be patient while the chart is being deployed **

{{- if .Values.diagnosticMode.enabled }}
The chart has been deployed in diagnostic mode. All probes have been disabled and the command has been overwritten with:

  command: {{- include "common.tplvalues.render" (dict "value" .Values.diagnosticMode.command "context" $) | nindent 4 }}
  args: {{- include "common.tplvalues.render" (dict "value" .Values.diagnosticMode.args "context" $) | nindent 4 }}

Get the list of pods by executing:

  kubectl get pods --namespace {{ include "common.names.namespace" . | quote }} -l app.kubernetes.io/instance={{ .Release.Name }}

Access the pod you want to debug by executing

  kubectl exec --namespace {{ include "common.names.namespace" . | quote }} -ti <NAME OF THE POD> -- bash

In order to replicate the container startup scripts execute this command:

  /opt/bitnami/grafana-alloy/bin/alloy run /opt/bitnami/grafana-alloy/config/config.alloy --storage.path=/tmp/alloy

{{- else }}

To verify that Grafana Alloy has started, run:

  kubectl get all -l "app.kubernetes.io/name={{ include "common.names.name" . }},app.kubernetes.io/instance={{ .Release.Name }}"

To access the dashboard site from outside the cluster follow the steps below:

{{- if .Values.ingress.enabled }}

1. Get the dashboard URL and associate its hostname to your cluster external IP:

    export CLUSTER_IP=$(minikube ip) # On Minikube. Use: `kubectl cluster-info` on others K8s clusters
    echo "Dashboard URL: http{{ if .Values.ingress.tls }}s{{ end }}://{{ .Values.ingress.hostname }}/"
    echo "$CLUSTER_IP  {{ .Values.ingress.hostname }}" | sudo tee -a /etc/hosts

{{- else }}

1. Get the dashboard URL by running these commands:

{{- if contains "NodePort" .Values.service.type }}
    export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath="{.spec.ports[0].nodePort}" services {{ include "common.names.fullname" . }})
    export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath="{.items[0].status.addresses[0].address}")
    echo "HTTP port at $NODE_IP:$NODE_PORT"
{{- else if contains "LoadBalancer" .Values.service.type }}
     NOTE: It may take a few minutes for the LoadBalancer IP to be available.
           You can watch the status of by running 'kubectl get --namespace {{ .Release.Namespace }} svc -w {{ include "common.names.fullname" . }}'
    export SERVICE_IP=$(kubectl get svc --namespace {{ .Release.Namespace }} {{ include "common.names.fullname" . }} -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
    echo "HTTP port at $SERVICE_IP:{{ .Values.service.ports.http }}"
{{- else if contains "ClusterIP" .Values.service.type }}
    echo "HTTP port at 127.0.0.1:2020"
    kubectl port-forward svc/{{ include "common.names.fullname" . }} 2020:{{ .Values.service.ports.http }} &
{{- end }}

{{- end }}

{{- end }}

{{- include "grafana-alloy.validateValues" . }}
{{- include "grafana-alloy.checkRollingTags" . -}}
{{- include "common.warnings.resources" (dict "sections" (list "alloy" "configReloader" ) "context" $) }}
{{- include "common.warnings.modifiedImages" (dict "images" (list .Values.alloy.image .Values.configReloader.image ) "context" $) }}
{{- include "common.errors.insecureImages" (dict "images" (list .Values.alloy.image .Values.configReloader.image ) "context" $) }}
